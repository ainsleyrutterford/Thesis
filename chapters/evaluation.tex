This chapter first discusses the experiments carried out in order to both improve the performance, and gain a better understanding, of the sub-components implemented. Experiments such as hyperparameter optimisation and ablation studies including both the two dimensional and three dimensional models are discussed. Once the results are cross-validated, the final results achieved by the optimised models are then presented, interpreted, and compared to results achieved by alternative models. Finally, various aspects of the project are critically evaluated.

\section{Two Dimensional Experimentation}

This section reintroduces the initial results achieved by the ``baseline'' 2D model implemented in Chapter \ref{chap:implementation} and then outlines the experiments carried out in an attempt to improve the performance both qualitatively and quantitatively.

\subsection{Initial Results}

\begin{figure}[!b]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.9]
            \begin{axis}[
                height=\axisdefaultheight,
                ylabel=\small{Loss},
                xlabel=\small{Epochs},
                ytick={0.15, 0.10},
                yticklabels={0.15, 0.10},
                grid=major,
                legend pos=north east,
                legend cell align=left,
                legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
                \addplot[blue, mark=x] table [x=xs, y=tl, col sep=comma] {csv/base.csv};
                \addlegendentry{\small{Train Loss}}
                \addplot[magenta, mark=x] table [x=xs, y=vl, col sep=comma] {csv/base.csv};
                \addlegendentry{\small{Val Loss}}
            \end{axis}
        \end{tikzpicture}
        \caption{Loss curve}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.9]
            \begin{axis}[
                height=\axisdefaultheight,
                ylabel=\small{Accuracy (\%)},
                xlabel=\small{Epochs},
                grid=major,
                legend pos=south east,
                legend cell align=left,
                legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
                \addplot[blue, mark=x] table [x=xs, y=ta, col sep=comma] {csv/base.csv};
                \addlegendentry{\small{Train Accuracy}}
                \addplot[magenta, mark=x] table [x=xs, y=va, col sep=comma] {csv/base.csv};
                \addlegendentry{\small{Val Accuracy}}
            \end{axis}
        \end{tikzpicture}
        \caption{Loss curve}
    \end{subfigure}
    \caption{The loss and accuracy curves achieved when training the model over 20 epochs using the hyperparameters outlined in Table \ref{tab:initialhyperparams}. It is important to note that although the validation loss begins to increase after epoch 13, the validation accuracy does not begin to decrease. Upon visual inspection of the boundary predictions, it could also be seen that the predictions were qualitatively better when the network was trained for 20 epochs rather than 13. The desirable qualitative features of a prediction are outlined in Figure \ref{fig:goodbad}.}
    \label{fig:basetrainacc}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.38\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/good-initial.png}
        \caption{Higher accuracy: 96.8\% and 96.5\%}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.58\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/bad-initial.png}
        \caption{Lower accuracy: 92.1\%, 88.4\%, and 91.5\%}
    \end{subfigure}
    \caption{Examples of both high and low quality outputs from the network trained using the initial hyperparameters outlined in Table \ref{tab:initialhyperparams}. The top images are patches from the validation set, the centre images are the corresponding ground truth labels, and the bottom images are the outputs of the network after they have been skeletonized. \textbf{(a)} Examples of high quality predictions. These examples achieved higher accuracies compared to the rest of the samples in the validation set. It can be seen that the boundaries contain comparatively less ``noise'' than the lower quality examples. \textbf{(b)} Examples of low quality predictions. The first example achieves a lower accuracy score due to the noise present in inner section of the coral on the left side of the image. The second example received a particularly low accuracy score due to the accidental classification of the object in the top left of the image as a boundary. This object is the platform that the coral sample is placed on in the CT machine and should perhaps have been cropped from the slices that these patches were produced from. The third example receives a low accuracy due to the accidental misclassification of a boundary perpendicular to the skeleton's surface. As discussed in Chapter \ref{chap:context}, density banding boundaries should be parallel to the coral skeleton surface.}
    \label{fig:goodbad}
\end{figure}

The initial results achieved by the baseline model can be discussed further in order to gain a better understanding of the strengths and weaknesses of the model. The loss and accuracy curves are shown in Figure \ref{fig:basetrainacc} and example boundary predictions of both high and low ``quality'' are shown in Figure \ref{fig:goodbad}. Their quality is assessed both via the accuracy metric achieved and via visual inspection. Looking at Figure \ref{fig:basetrainacc}, it can be see that the accuracy achieved on the validation set is noticeably lower than the accuracy achieved on the training set. This unusual behaviour may be due to the validation set containing ``easier'' examples in which the annual banding is more obvious. Although the performance reported on this validation set may potentially be positively skewed, the final performance achieved by the model will be reported on cross-validated results and so the selection biases that arise from this dataset split should not affect the final reported performance.

\subsection{Hyperparameter optimisation}

As discussed in Chapter \ref{chap:technical}, the performance achieved by deep neural networks is known to depend critically on the identification of a good set of hyperparameters~\cite{hyperparam, goodhyperparam}. In this project, a manual form of grid search was used to discover the optimal set of hyperparameter configurations. In this technique, all hyperparameters are fixed and only a single hyperparameter is varied at a time. Although this may not be the most efficient approach, it enables a better understanding of the model to be gained.

% Due to the time constraints of the project, it would not be possible to perform an exhaustive search of all possible hyperparameter configurations. Thus, the primary aim of this chapter is not to achieve the best performance possible, but instead to evaluate which hyperparameters affect the performance most and reason as to why this is the case.

\subsubsection{Learning Rate}

Of all the hyperparameters relevant to deep learning models, the optimisation of the learning rate often has the biggest impact on the performance of a model~\cite{bengio2012practical}. The selection of a learning rate too large can cause an optimisation algorithm to take a step ``over'' minima causing the loss to inadvertently increase rather than decrease. The selection of a learning rate too small can also hinder performance as the optimisation algorithm may become permanently stuck in a suboptimal local minimum~\cite{goodfellow}.

Since it is usually not possible to calculate an optimal learning rate a priori~\cite{neuralbook}, some form of trial and error is required. A reasonable range of values to experiment with are given by Bengio in~\cite{bengio2012practical} and will be used as the basis of the values experimented with in this section. Bengio suggests an initial learning rate within the range of $10^{-6}$ to $1$.

The accuracies achieved when training the network using various initial learning rates in this range are shown in Figure \ref{fig:lrplot}. It can be seen that the final average accuracy achieved after 20 epochs is similar for all but the 0.00001 value. Although it may look like the accuracy for this learning rate could still improve with further training, the accuracy does not improve further even after 40 epochs. This plateau in the accuracy achieved suggests that the 0.00001 learning rate may be low enough to become stuck in suboptimal local minima.

The training loss achieved by the 0.001 learning rate oscillated at ${\sim}0.5$ for the entire training process, even over multiple training attempts. In comparison, the 0.00005 learning rate achieved a training loss of 0.06 and a validation loss of 0.16. These oscillations are a common sign of a learning rate being too high and stepping ``over'' minima~\cite{bishop1995neural}. Pure black images were output for all samples in both the training and validation set resulting in an accuracy of 0\% being achieved.

Although the both the 0.00005 and 0.0001 learning rates all achieved a similar accuracy of ${\sim}96\%$ after 20 epochs, the 0.00005 value was ultimately chosen for multiple reasons. First, the training and validation accuracy curves are smoother than the curves resulting from higher values, allowing the accuracy to be more reliably used as a stopping criteria for early stopping. Second, when inspecting the results visually, the 0.00005 learning rate results in predictions that are far clearer in the inner areas of the coral skeleton than any other learning rate. Although most of the learning rates were able to produce acceptable predictions nearer to the surface of the coral, only the 0.00005 learning rate was able to produce acceptable predictions in these inner areas. An example of a patch that benefited from the 0.00005 learning rate is shown in Figure \ref{fig:lrdiff}.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/lr-comparison.png}
        \caption{Ground truth boundaries}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/1e4-example.png}
        \caption{$\eta=0.0001$ prediction}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/5e5-example.png}
        \caption{$\eta=0.00005$ prediction}
    \end{subfigure}
    \caption{An example of a patch that benefits from the a learning rate $\eta$ of 0.00005. The predictions shown have been skeletonized using the technique discussed in Chapter \ref{chap:implementation}. The $\eta=0.0001$ prediction achieved an accuracy of 92.1\% whereas the $\eta=0.00005$ prediction achieved an accuracy of 95.5\%. The 0.0001 prediction achieves a lower accuracy score due to the noise present in inner section of the coral on the left side of the image. The 0.00005 prediction has more realistic predictions in this area of the image with all boundaries seeming close to parallel to the growth surface of the skeleton.}
    \label{fig:lrdiff}
\end{figure}

\begin{figure}[!t]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.9]
            \begin{axis}[
                height=\axisdefaultheight,
                ylabel=\small{Accuracy (\%)},
                xlabel=\small{Epochs},
                grid=major,
                legend pos=south east,
                legend cell align=left,
                legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
                \addplot[magenta, mark=x] table [x=xs, y=lr00001_vacc, col sep=comma] {csv/lr.csv};
                \addlegendentry{\small{$\eta=0.00001$}}
                \addplot[darkgray, mark=x] table [x=xs, y=lr00005_vacc, col sep=comma] {csv/lr.csv};
                \addlegendentry{\small{$\eta=0.00005$}}
                \addplot[blue, mark=x] table [x=xs, y=lr0001_vacc, col sep=comma] {csv/lr.csv};
                \addlegendentry{\small{$\eta=0.0001$}}
                \addplot[orange, mark=x] table [x=xs, y=lr0005_vacc, col sep=comma] {csv/lr.csv};
                \addlegendentry{\small{$\eta=0.0005$}}
            \end{axis}
        \end{tikzpicture}
        \caption{Varying learning rate}
        \label{fig:lrplot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.9]
            \begin{axis}[
                height=\axisdefaultheight,
                ylabel=\small{Accuracy (\%)},
                xlabel=\small{Epochs},
                grid=major,
                legend pos=south east,
                legend cell align=left,
                legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
                \addplot[magenta, mark=x] table [x=xs, y=bs1_vacc, col sep=comma] {csv/batch.csv};
                \addlegendentry{\small{$batch=1$}}
                \addplot[darkgray, mark=x] table [x=xs, y=bs2_vacc, col sep=comma] {csv/batch.csv};
                \addlegendentry{\small{$batch=2$}}
                \addplot[blue, mark=x] table [x=xs, y=bs4_vacc, col sep=comma] {csv/batch.csv};
                \addlegendentry{\small{$batch=4$}}
                \addplot[teal, mark=x] table [x=xs, y=bs8_vacc, col sep=comma] {csv/batch.csv};
                \addlegendentry{\small{$batch=8$}}
                \addplot[orange, mark=x] table [x=xs, y=bs16_vacc, col sep=comma] {csv/batch.csv};
                \addlegendentry{\small{$batch=16$}}
            \end{axis}
        \end{tikzpicture}
        \caption{Varying batch size}
        \label{fig:bsplot}
    \end{subfigure}
    \caption{The accuracies achieved on the validation set using various hyperparameter values. A TensorBoard smoothing value of 0.3 was used to improve visibility. \textbf{(a)} The accuracies achieved when training the network using various initial learning rate values $\eta$. Apart from the learning rate, the same hyperparameters used to achieve the initial results (outlined in Table \ref{tab:initialhyperparams}) were used once again. \textbf{(b)} The accuracies achieved when training the network using various batch sizes. Apart from a learning rate of 0.00005 being used, the hyperparameters outlined in Table \ref{tab:initialhyperparams} were used once again.}
\end{figure}

\subsubsection{Batch Size}

The next parameter experimented with was the batch size. Ranging from a size of one up to a few hundreds in some applications~\cite{bengio2012practical}, the batch size not only affects the performance achieved by a network, but can also affect training times significantly.

Small batches with sizes equal to powers of two were experimented with. The accuracies achieved when training using these various batch sizes are shown in Figure \ref{fig:bsplot}. Looking at Figure \ref{fig:bsplot}, it can already be seen that as the batch size increases, the accuracy achieved by the model decreases (with the exception of a batch size of one). Batch sizes greater than 16 were also experimented with but the accuracy achieved only continued to decrease. It may seem that the accuracy achieved still increase further given more training epochs, however even after 40 epochs the network trained with $batch=8$ only achieved an accuracy of 91\% compared to the accuracy of 95\% achieved when $batch=2$. Note that in order to ensure that the model was exposed to the same number of samples per epoch, with each increase in batch size, the \texttt{steps-per-epoch} argument of the \texttt{fit} method was also decreased. For example, if the batch size was doubled, the \texttt{steps-per-epoch} was halved. This is due to the \texttt{steps-per-epoch} argument specifying how many batches compose an epoch, rather than how many samples compose an epoch.

In practice, it has often been observed that larger batch sizes result in lower accuracy being achieved in deep learning models~\cite{keskar2016large, smallbatch, largebatch}. Keskar et al.\ propose that this decrease in performance may be attributed to larger batch sizes converging to ``sharp'' minima~\cite{keskar2016large}. Whilst a ``flatter'' minimum can be described with low precision, a sharp minimum requires high precision. Keskar et al.\ cite the minimum description length (MDL) theory, which states that statistical models that require fewer bits to describe generalise better~\cite{rissanen}, and argue that since flat minima can be specified with lower precision, they tend to have better generalisation performance. Note that the accuracy achieved on the validation set is a measure of how well the network generalises, since the network has not yet been trained on any slices present in the validation set.

Ultimately, a batch size of two was decided upon as the network trained with this size achieved the highest training and validation accuracy. Although a batch size of 16 provided a ${\sim}1.4$ times speed up in the training process, the model still trains in under 20 minutes\footnote{When trained using an Nvidia P100 ``Pascal'' GPU.} when using a batch size of two. For now, the improved performance provided by a smaller batch size outweighs the benefits provided by the training time saved when using larger sizes. If vast amounts of labelled data were available, a larger batch size would likely be necessary to keep training times reasonable.

\subsubsection{Optimisation Algorithms}

This section discusses experimention carried out with the stochastic gradient descent (SGD) optimisation algorithm. SGD was used successfully to train the original U-Net architecture to perform semantic segmentation on 2D greyscale biomedical data~\cite{ronneberger2015u} and so is a reasonable alternative to the Adam optimiser used so far.

The Keras implementation of SGD using learning rates of 0.00005, 0.0001, 0.0005, 0.001, 0.005, and 0.01 were experimented with but in all cases no acceptable results were produced. When trained with each of these learning rates, the losses and accuracies for both the training and validation sets remained almost constant from the first epoch onwards and the produced predictions resembled the input images more than the corresponding labels. Examples of the predictions produced are shown in Figure \ref{fig:sgdpredictions}.

In an attempt to improve the performance achieved by SGD, various levels of the Keras implementation of classical momentum~\cite{polyak} were experimented with. When momentum is enabled, the weight updates determined by SGD are calculated as a linear combination of the gradient and the previous updates made~\cite{momentum}. Momentum values of 0.1 to 0.9 in combination with all of the learning rates mentioned previously were experimented with but no acceptable results were produced once again. Although no acceptable results were produced, it is important to note that when using a momentum value above 0.5 in combination with a learning rate above 0.001, the validation loss was no longer constant, and a typical loss curve was produced. This suggests that the momentum implemented in Adam may play an important roll in allowing the training loss to fall as the training process continues.

Although the SGD optimiser was utilised successfully in the original U-Net paper, the attempts to train this implementation of the architecture were not successful. Thus, the Adam optimiser continued to be used for the remainder of the experiments.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/sgd1.png}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/sgd5.png}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/sgd3.png}
    \end{subfigure}
    \caption{Example predictions output by the network when trained using the SGD optimisation algorithm using a learning rate of 0.0001. Predictions almost identical to these were output by all learning rates when a momentum value lower than 0.5 was used. It appears as though the input's channel has been inverted and the image has been blurred slightly.}
    \label{fig:sgdpredictions}
\end{figure}

\subsubsection{Loss Functions}

As outlined in Chapter \ref{chap:implementation}, with ${\sim}$98\% of an average labelled patch being black (part of the ``not part of a boundary'' class), the utilised labelling technique gives rise to a severe class imbalance. As a result, a model that learns to output a pure black image would achieve a low loss value for ${\sim}$98\% of the image. Although this has proved to be a problem when assessing the accuracy of the model, the effects of this class imbalance on the network's ability to learn are not yet obvious. This section outlines the experiments carried out involving the focal loss function~\cite{focalloss} designed specifically to address the class imbalance problem.

The focal loss introduces two tunable parameters: a weighting factor $\alpha \in [0, 1]$ used to increase the loss produced by the misclassification of minority class samples, and a ``focusing'' parameter $\gamma \geq 0$ used to reduce the contribution to the loss that ``easy'' examples have~\cite{focalloss}. When the recommended default values of $\gamma=2$ and $\alpha=0.25$ were used, the network produced predictions comparable to those produced using cross-entropy both qualitatively and quantitatively with a validation accuracy of 93\% being achieved. When varying the the $\alpha$ and $\gamma$ parameters, the best accuracy of 94\% was achieved with $\alpha=0.25$ and $\gamma=1$.

When varying the $\gamma$ parameter, an interesting phenomena was observed. Looking at Figure \ref{fig:lossfunctiondiff}, it can be seen that an increase in $\gamma$ results in more pixels being classified as the minority class---the ``part of a boundary'' class. Since the loss value produced per correctly classified black pixel is lower as $\gamma$ increases, the contribution of an incorrectly classified white pixel to the overall loss is far higher. As a result, the network values the correct classification of a white pixel more than the correct classification of a black pixel and so produces more white pixels overall.

Since even the optimal parameterisation of the focal loss did not yield any improvements over the cross-entropy loss, the cross-entropy loss was ultimately decided upon. The hyperparameters used for the remainder of the experiments are summarised in Table \ref{tab:hyperparams1}.

\begin{table}[t]
    \centering
    \caption{A summary of the training hyperparameters decided upon as a result of the various experiments run.}
    \begin{tabular}{@{}ll@{}}
    \toprule
    Hyperparameter   & Setting      \\ \midrule
    Architecture     & 2D U-Net   \\
    Optimisation Algorithm & Adam \\
    Learning rate & 0.00005 \\
    Loss function & Cross-entropy \\
    Epochs & 20 \\
    Steps per epoch & 500 \\
    Batch size & 2 \\ \bottomrule
    \end{tabular}
    \label{tab:hyperparams1}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/focal_g01.png}
        \caption{$\gamma=0.1$}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/focal_g2.png}
        \caption{$\gamma=2$}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/focal_g5.png}
        \caption{$\gamma=5$}
    \end{subfigure}
    \caption{An illustration of how the focusing parameter $\gamma$ of the focal loss function affects the predictions output by the network. Although Lin et al.\ suggest increasing $\alpha$ to 0.75 when $\gamma$ is decreased to 0.1, $\alpha$ was set to a constant value of 0.25 for all values of $\gamma$ to better illustrate how $\gamma$ affects the performance of the network. It can be seen that as $\gamma$ increases, the percentage of pixels that are predicted as the ``part of a boundary'' class also increases.}
    \label{fig:lossfunctiondiff}
\end{figure}

\subsubsection{Dropout}

Dropout is a regularization technique proposed by Srivastava et al.~\cite{dropout} in 2014. As mentioned in Chapter \ref{chap:implementation}, although the original U-Net architecture does not mention any dropout layers, two dropout layers were placed in this implementation: one after the last contracting block and one after the bottleneck block. The dropout \texttt{rate} arguments that specify the probability that a neuron is dropped were initially set to 0.5 but this value is experimented with and discussed further in this section.

The training and validation loss curves resulting from various dropout probabilities are shown in Figure \ref{fig:dropoutplots}. It can be seen that the dropout probability affects the training and validation losses in a similar manner to that discovered by Srivastava et al.; the higher the dropout probability, the higher the training loss and the lower the validation loss~\cite{dropout}.

Similarly to the training loss, the training accuracy also improved as $P$ decreased. However, the validation accuracy was not directly correlated to $P$; the validation accuracies achieved when $P$ was equal to 0.1, 0.3, 0.5, 0.7, and 0.9, were 92\%, 94\%, 95\%, 93\%, and 91\% respectively. Since the validation accuracy was highest when a dropout rate of 0.5 was used, the use of this rate was decided upon going forward.

\begin{figure}[t]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.9]
            \begin{axis}[
                height=\axisdefaultheight,
                ylabel=\small{Loss},
                xlabel=\small{Epochs},
                grid=major,
                ytick={0.16,0.14,0.12,0.10,0.08,0.06},
                yticklabels={0.16,0.14,0.12,0.10,0.08,0.06},
                legend pos=north east,
                legend cell align=left,
                legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
                \addplot[magenta, mark=x] table [x=xs, y=0_tloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.1$}}
                \addplot[darkgray, mark=x] table [x=xs, y=3_tloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.3$}}
                \addplot[blue, mark=x] table [x=xs, y=5_tloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.5$}}
                \addplot[teal, mark=x] table [x=xs, y=7_tloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.7$}}
                \addplot[orange, mark=x] table [x=xs, y=9_tloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.9$}}
            \end{axis}
        \end{tikzpicture}
        \caption{Training loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.9]
            \begin{axis}[
                height=\axisdefaultheight,
                ylabel=\small{Loss},
                xlabel=\small{Epochs},
                grid=major,
                ytick={0.20,0.18,0.16,0.14,0.12},
                yticklabels={0.20,0.18,0.16,0.14,0.12},
                legend pos=north east,
                legend cell align=left,
                legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
                \addplot[magenta, mark=x] table [x=xs, y=0_vloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.1$}}
                \addplot[darkgray, mark=x] table [x=xs, y=3_vloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.3$}}
                \addplot[blue, mark=x] table [x=xs, y=5_vloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.5$}}
                \addplot[teal, mark=x] table [x=xs, y=7_vloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.7$}}
                \addplot[orange, mark=x] table [x=xs, y=9_vloss, col sep=comma] {csv/drop.csv};
                \addlegendentry{\small{$P=0.9$}}
            \end{axis}
        \end{tikzpicture}
        \caption{Validation loss}
    \end{subfigure}
    \caption{The loss achieved on the training and validation sets when training the network with varying dropout probabilities $P$. A TensorBoard smoothing value of 0.3 was used to improve visibility.}
    \label{fig:dropoutplots}
\end{figure}

\subsection{Resolution}

In order to gain a better understanding of the U-Net architecture, the resolution of the patches input to the network was experimented with. These experiments could 

Even with far more data for smaller patches, the performance was not good. suggests that the network needs context in order to perform well.

larger slices just did not have enough data but should probably work better using same logic as above?

\subsection{Augmentation}
\label{sec:evalaugmentation}

\subsubsection{No Augmentation}

I said in Chapter \ref{chap:implementation} that augmentation was very important.

\subsubsection{Augmentation Ranges}

Would probably not have been as important with larger amounts of data as the dataset would represent a more comprehensive range of possible samples.

\subsection{Ablation Studies}

comparison of convtranspose to upsample and then conv.

remove the relu at the end of the implementation?

Make all max-poolings just 1x1? so no max pooling.

Make all blocks just 1 conv layer instead of 2?

Remove bottleneck?

Halve all of the output feature channel numbers?

Maybe visualise ablations with a graph showing each ablation with accuracy achieved or something? like in that paper. or a table?

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[scale=0.9]
        \begin{axis}[
            height=\axisdefaultheight,
            width=0.6\textwidth,
            ylabel=\small{Accuracy (\%)},
            xlabel=\small{Epochs},
            grid=major,
            legend pos=south east,
            legend cell align=left,
            legend style={fill=white, fill opacity=0.8, draw=none,text opacity=1}]
            \addplot[magenta, mark=x] table [x=xs, y=lr00001_vacc, col sep=comma] {csv/lr.csv};
            \addlegendentry{\small{$\eta=0.00001$}}
            \addplot[darkgray, mark=x] table [x=xs, y=lr00005_vacc, col sep=comma] {csv/lr.csv};
            \addlegendentry{\small{$\eta=0.00005$}}
            \addplot[blue, mark=x] table [x=xs, y=lr0001_vacc, col sep=comma] {csv/lr.csv};
            \addlegendentry{\small{$\eta=0.0001$}}
            \addplot[orange, mark=x] table [x=xs, y=lr0005_vacc, col sep=comma] {csv/lr.csv};
            \addlegendentry{\small{$\eta=0.0005$}}
        \end{axis}
    \end{tikzpicture}
    \caption{Ablation}
    \label{fig:ablationplot}
\end{figure}

\section{Three Dimensional Experimentation}

\subsection{Hyperparameter optimisation}

Due to the time constraints of the project, far fewer hyperparameters were optimised for the three dimensional architectures.

Briefly mention that various learning rates and batch sizes were experimented with but none produced better results (unless then do?) and the new optimal hyperparameters for the 2D arch were used.

\subsection{Alternative Modified Architectures}

\subsubsection{Increased 3D Convolution Output Channels}

As mentioned in Chapter \ref{chap:implementation}, the number of feature channels output by the 3D convolutional layers was chosen in order to replicate 

Varying numbers of output channels in Conv3D layers as you said youd experiment with this.

\subsubsection{Extended 2D U-Net}

The version with a few 4d convs on the front and then outputting to a 2D (9 channels or 1) image to put straight into U-Net. May not have worked due to vanishing gradients? its right at the beginning of the network?

\subsubsection{Fully Three Dimensional Architecture}

Lack of data. Was not consistently labelled. Show adjacent slices. Proves how hard it is to label images.

\section{Cross-validation}
\label{sec:evalcrossval}

As outlined in Chapter \ref{chap:technical}, cross-validation is designed to give a comprehensive measure of a model's performance throughout the entire dataset, rather than just a particular subset. Cross-validation techniques reduce the effects of selection bias on the reported performance. Since the slices composing the test set and the slices composing the training set were extracted from different samples, some form of selection bias is inevitable and the performance achieved on the current test set does not necessarily represent the performance that would be achieved on the entire dataset.

\subsection{Train/test Splits}

In order to perform cross-validation, the dataset must be split into multiple training and testing sets known as ``train/test'' splits. Similarly to how the dataset was split in Chapter \ref{chap:implementation}, the patches composing the test sets must all be produced from slices of coral skeletons that are not part of the corresponding training sets. This ensures that in each train/test split, the network can not be overfitting to the nature of the annual banding present in the skeletons used for testing. If this had been the case, the performance on each test set could have been positively skewed. For this reason the train/test splits were manually selected as opposed to randomly selected as they usually can be. It is important to note that no hyperparameters or augmentation settings were changed during the cross-validation process as this could have been used to influence the final reported performance in some way.

Since all of the data was extracted from four coral skeletons, four splits were selected. In each split, the test set consists of every patch produced from one skeleton, and the training set consists of every patch produced from the rest of the available skeletons. The manual splits chosen are outlined in Tables \ref{tab:appendixtab}--\ref{tab:appendixtab4}.

Maybe mention anything interesting found here?

\begin{table}[!t]
\centering
\caption{Cross validation}
\input{tables/cross-val}
\label{tab:crossval}
\end{table}

\section{Final Results}

\begin{figure}[!p]
    \centering
    \includegraphics[width=\textwidth, height=1.45\textwidth]{example-image-b}
    \caption{A full page of final results achieved.}
    \label{fig:finalresults}
\end{figure}

\section{Comparisons with Other Architectures}

Compare SegNet, U-Net, pix2pix. Maybe find somewhere to put in the pix2pix generation for fun? If not don't worry. Do mention pix2pix using different backbones or whatever its called and why you think it didnt perform well.

\subsection{SegNet}

\subsection{pix2pix}



\section{Critical Evaluation}

\subsection{Boundary Extraction}

\subsubsection{Labelling}

Could have chosen the thick style as this would get rid of the class imbalance and would allow for a normal accuracy metric etc. Mention again how hard it is to be consistent.

\subsubsection{Lack of Data}

very hard for a non expert to label.

\subsubsection{3D Architectures}

Augmentation proved so important. Perhaps more transformations should have been implemented.

\subsubsection{Further Possible Experiments}

Spatial Dropout?

L2 Regularization

Batch Normalization

\subsection{Accuracy Metric}

Can't tell which boundary it should be looking for. a completely white image would produce 100\% accuracy but the skeletonization makes the amount of white pixels low.

Maybe show concrete examples of where it falls down? images shifted to the right so that one boundary is roughly where another should be? images that have many white lines that are even perpendicular to the right labels should still produce a pretty low score?

Show image produced by SGD. completely wrong but still gets 60\% due to the white part in the bottom left of the image. this might be fixed now that we use a max of 90?

visual inspection is still needed take for example the learning rate plots, they all achieve accuracies within a percentage of each other but some are clearly worse than others?

\subsection{Calcification Rate Estimation}

\section{Comparisons with Existing Techniques}

Erica sent a paper a while ago that used a computer program to calculate the calcification rate  \url{https://www.geosociety.org/datarepository/2015/2015015.pdf} check the email from her with subject DeCarlo Paper

% {\bf A topic-specific chapter, of roughly $15$ pages} 
% \vspace{1cm} 

% \noindent
% This chapter is intended to evaluate what you did.  The content is highly 
% topic-specific, but for many projects will have flavours of the following:

% \begin{enumerate}
% \item functional  testing, including analysis and explanation of failure 
%       cases,
% \item behavioural testing, often including analysis of any results that 
%       draw some form of conclusion wrt. the aims and objectives,
%       and
% \item evaluation of options and decisions within the project, and/or a
%       comparison with alternatives.
% \end{enumerate}

% \noindent
% This chapter often acts to differentiate project quality: even if the work
% completed is of a high technical quality, critical yet objective evaluation 
% and comparison of the outcomes is crucial.  In essence, the reader wants to
% learn something, so the worst examples amount to simple statements of fact 
% (e.g., ``graph X shows the result is Y''); the best examples are analytical 
% and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
% contradicts [1], which may be because I use a different assumption'').  As 
% such, both positive {\em and} negative outcomes are valid {\em if} presented 
% in a suitable manner.