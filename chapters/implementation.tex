This chapter details the implementation of the automated solution proposed in Chapter \ref{chap:context}. First, an overview of the various components implemented throughout the project is given. The implementation of a deep network capable of automatically extracting annual density bands present in two dimensional data is then discussed. The attempts at implementing a network capable of automatically extracting the annual density bands present in three dimensions are also detailed. Then, the implementation of and reasoning behind the custom accuracy metric is discussed. Finally, the techniques used to estimate the calcification rate from the extracted boundaries are described.

\section{Overview}

A system capable of calculating the calcification rate given only a small section of density data requires the implementation of many separate sub-components. Figure \ref{fig:overview} provides an overview of these components and outlines the order in which they were implemented and will be discussed. Ultimately, the training of a 2D architecture to predict the positions of the boundaries between annual density bands was the main focus of the project. The steps taken to implement this component compose the majority of the content in this Chapter.

All aspects of the project relied heavily on the provided dataset. The processing and labelling of the data was a significant part of the implementation and will be discussed in detail. An example of a 3D scan present in the dataset is shown in Figure \ref{fig:scanexample}.

\begin{figure}[!b]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/real-coral.png}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/3D-scan.png}
    \end{subfigure}
    \caption{\textbf{(left)} An example of a Porites coral skeleton from the Natural History Museum's collection. This particular sample was collected from the Solomon Islands in 1974. In the initial dataset, the scan of this sample is represented by 1945$\times$1508$\times$1208 voxels. \textbf{(right)} A 3D volume rendering of the CT scan of the sample shown on the left. Created using the Avizo software package.}
    \label{fig:scanexample}
\end{figure}

\begin{figure}[!p]
    \centering
    \includegraphics[width=\textwidth]{images/overview.pdf}
    \caption{A diagram providing an overview of the components implemented and discussed throughout the project. The experimentation is discussed in Chapter \ref{chap:evaluation}. The dropout illustration in the experimentation section is taken from~\cite{dropout}.}
    \label{fig:overview}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/extraction.png}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/slice-example.png}
    \end{subfigure}
    \caption{An illustration of the how the 3D CT data is composed of many 2D slices. \textbf{(left)} A 3D volume rendering of one of the CT scans present in the dataset. The grey rectangle ``cuts'' the scan and produces the slice shown on the right. It can also be seen that the scan on the left is represented by many slices similar to the one shown on the right. \textbf{(right)} A 2D slice extracted from the CT scan shown on the left. The slice is a negative; brighter pixels correspond to high density and darker pixels correspond to low density.}
    \label{fig:extraction}
\end{figure}

\section{Two Dimensional Boundary Extraction}

This section outlines the steps taken to implement and train a CNN capable of extracting the annual density banding present in two dimensional data.

\subsection{Labelling the Data}
\label{sec:2dlabel}

In order for a supervised CNN to perform well, large amounts of labelled data must be available to train on. Since the dataset provided was initially unlabelled, a manual labelling process was devised and is described in this section.

\subsubsection{The Initial Dataset}

The dataset provided contains more than 160 three dimensional computed tomography (CT) scans of unique coral skeletons from the Natural History Museum's collection. Each individual 3D scan consists of stacks of thousands of 2D \texttt{.tif} images which will be referred to as ``slices'' (see Figure \ref{fig:extraction}). This unlabelled dataset will be referred to as the ``initial dataset''. Although the initial dataset contains scans of ten different genera of coral, only scans of the Porites genus were considered for labelling and training the network with. There are 58 scans of Porites skeletons in total. An example of a Porites skeleton that is part of the dataset was shown previously in Figure \ref{fig:scanexample}. The Porites scans were chosen as they contain annual banding that can be more easily identified and labelled when compared to other coral genera.

A typical scan consists of ${\sim}2000$ slices that each have the same resolution of ${\sim}$2000$\times$2000 pixels resulting in an overall 3D resolution of 2000$\times$2000$\times$2000 voxels. However, note that the 3D resolution, scale (e.g., the number of voxels used to represent a centimetre cubed), and density calibration (e.g., the voxel value used to represent a given density) of each scan varies.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/rough-label.png}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/smooth-label.png}
    \end{subfigure}
    \caption{A comparison of the two methods of labelling that were initially considered. The boundary labels are coloured red and placed over the same original negative 2D slice. \textbf{(left)} A method of labelling in which the boundaries were placed at the sharpest change in density resulting in complex boundaries whose positions are highly sensitive to noise in the image. \textbf{(right)} A method in which the chosen boundaries are ``smooth'' whilst also being placed as close as possible to the sharpest changes in density.}
    \label{fig:labelstyle}
\end{figure}

\subsubsection{Slice Selection and Extraction}

Not all slices that compose a scan contain annual density banding that can confidently be identified. In order to find appropriate slices, each of the 58 Porites scans were opened and inspected manually using a 3D visualisation program called Avizo\footnote{\url{https://tiny.cc/avizo}}. Avizo allows users to view slices orthogonal to the $x$, $y$, or $z$ axis of a scan. A selection of six slices that could confidently be labelled were chosen. Since the density banding boundaries are orthogonal to the growth direction (or ``concordant'' with the growth surface~\cite{knutson}), the slices that present the most obvious density banding are normally parallel to the direction of growth. Once these slices were identified, a Python script was used to extract each slice and export them as greyscale \texttt{.png} images. The slices can be represented as greyscale images since they only require one channel to represent depth. Some of the surrounding slices were also extracted in order to curate the 3D dataset used later in the project (discussed in Section~\ref{sec:threedimension}).

\subsubsection{Slice Labelling}

Once the slices were selected, a labelling process was established. Initially, two methods of labelling were considered and shown to Dr Erica Hendy, a senior lecturer in biogeochemical cycles. Of the two methods (shown in Figure \ref{fig:labelstyle}), the ``smooth'' method was deemed as the more realistic choice. An idealised annual density cycle is actually in the form of a sinusoidal wave with the density gradually changing from high to low and back over the course of a year~\cite[p. 39]{coralsine}. Thus, an exact boundary between a high and low band does not actually exist, making the ``complex'' method unrealistic both in terms of reproducibility, and in terms of biological accuracy.

Each selected slice was then manually labelled using the GNU Image Manipulation Program (GIMP)\footnote{\url{https://www.gimp.org}}. A one pixel wide white line is drawn at the beginning and end of each annual high density band and the rest of the image is black---no values other than black or white are present in the labels. In order to ensure that the decisions made regarding the boundaries of the annual bands were as consistent as possible, each slice was manually labelled three or more times, and the most common boundaries were chosen. It is important for the labelling method to be consistent and reproducible as this enables the dataset to be more easily expanded in the future.

\subsection{Dataset Curation}
\label{sec:datasetcuration}

With only a small selection of slices successfully extracted and labelled, a larger dataset of image-label pairs was required for the model to train on.

\subsubsection{Sliding Window}

In order to expand the dataset, a sliding window technique was used. A script was written in Python that took a slice, two 2D coordinates, a ``stride'', and a size as arguments. The pair of coordinates corresponded to the top left corner and bottom right corner of the confidently labelled area of the slice. This was the area that the window would slide over in order to create the resulting ``patches''. The stride argument dictated how far the window should slide before each patch was produced. This sliding window technique is visualised in Figure \ref{fig:slidingwindow}. The most common stride value used was 20. The size argument defined how large the resulting patches should be. The patches initially produced had a resolution of 256$\times$256 pixels, but various sizes of patches were experimented with and are discussed in Chapter \ref{chap:evaluation}. The arguments used in order to produce the curated dataset used in this project are shown in Table \ref{tab:slidingwindow}. A total of 388 patches were produced from six slices taken from four different coral skeleton scans. This dataset of 388 256$\times$256 patches and their corresponding labels were stored as greyscale \texttt{.png} images.

In order to prevent the dataset from representing certain coral samples better than others, an effort was made to equalise the number of patches produced per scan. Due to different slices containing varying sizes of confidently labelled areas, the stride argument was used to increase or reduce the number of patches produced. However, it was also important to produce as much data as possible to enable the network to perform well. Ultimately, some slices produced almost five times as many slices as others resulting in some imbalance in the types of coral scans represented by the dataset. Although this could negatively affect the generalisability of a network trained on this dataset, cross-validation with carefully selected splits to represent each coral scan was ultimately used and is discussed in Chapter \ref{chap:evaluation}.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \node [anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=1\textwidth]{images/sliding-window.pdf}};
        \begin{scope}[decoration={
            markings, mark=at position 1 with {\arrow[scale=1.8]{stealth}}}] 
            \draw[postaction={decorate}, color={rgb:black,3}] (7.88,3)--(8.9,3);
        \end{scope}
    \end{tikzpicture}
    \caption{A visualisation of the sliding window technique used to produce multiple ``patches'' from a single slice. A portion of an original negative slice is shown with the manual labelling overlaid in white. The windows on the left show where the patches on the right would be produced from. $size$ is the patch size, and $s$ is the stride value. The windows have a width and height of $size$ and will be moved $s$ pixels to the right before another patch is produced. Once the window reaches the right hand side of the image, the window will be placed back at the start and will be shifted downwards by $s$ pixels before the process starts again.}
    \label{fig:slidingwindow}
\end{figure}

\subsubsection{Splitting the Dataset}

For the initial training and testing of the network, the curated dataset was split into a training set, a validation set, and a test set. Of the 388 patches, 322 were used for training, 56 for testing, and 10 for validation. The patches composing the test and validation sets were all produced from slices of a coral skeleton that was not part of the training set. This ensured that the network could not have been overfitting to the nature of the annual banding present in the skeleton used for testing. If this had been the case, the performance on the test data could have been positively skewed.

\subsection{Data Augmentation}

As discussed in Chapter \ref{chap:technical}, data augmentation is an important regularization technique. Since only 388 patches had been produced, extensive augmentation was used to artificially expand the dataset. This data augmentation later proved effective in improving the performance achieved by the network and is discussed further in Chapter \ref{chap:evaluation}.

\begin{table}[!t]
\centering
\caption{The arguments used with the Python script for each labelled slice. For each slice, a size argument of 256 was also specified. Note that some slices are represented by two rows as these slices contained two separate areas that could be confidently labelled. It was not possible to use one area that contains the two areas as this would result in multiple patches with no labelling being produced.}
\input{tables/curation}
\label{tab:slidingwindow}
\end{table}

\subsubsection{The Keras \texttt{ImageDataGenerator} class}

The Keras library allows users to easily implement augmentation using the \texttt{ImageDataGenerator} class\footnote{\url{https://keras.io/preprocessing/image}}. In this case, the \texttt{flow\_from\_directory} method was used to perform online augmentation. This method first loads images from a directory into batches. A series of random transformations are then applied to each batch. These original batches are then replaced with the new randomly transformed versions. The \texttt{flow\_from\_directory} method then returns a Python generator that will yield randomly augmented batches of the data indefinitely. A network trained on the generator produced by this method will therefore be trained on randomly transformed samples as opposed to the original samples themselves. The batches can then be input to the model in the form of $(batch \times y \times x \times 1)$ tensors where $batch$ is the batch size, $y$ and $x$ are the heights and widths of the patches respectively, and 1 is the number of channels (since the patches are stored as greyscale images).

The \texttt{flow\_from\_directory} method only loads one stream of images from a directory. In order to load the corresponding labels from another directory, a \texttt{flow\_from\_directory} method from another instance of the \texttt{ImageDataGenerator} class must be used. The two generators produced can then be ``zipped'' into a single generator that yields image-label pairs.

Since the \texttt{flow\_from\_directory} method applies random transformations to each image, the same random transformations must also be applied to the corresponding labels. This can be achieved by passing the same \texttt{seed} argument to the two \texttt{flow\_from\_directory} methods used to load the images and labels. These methods are also capable of shuffling the order in which images are loaded and trained with, so using the same \texttt{seed} value also ensures that the shuffling follows the same order for both the images and labels. The actual transformations to be used when augmenting the data are specified using various arguments passed to the \texttt{ImageDataGenerator} constructor.

Since the training generator yields randomly augmented image-label pairs indefinitely, a ``classic'' epoch will never be completed. Instead, when training the network the \texttt{steps-per-epoch} argument defines how many batches should be processed before an epoch is considered complete. A simplified example demonstrating the implementation of the training generator and the transformations used is shown in Listing \ref{lst:augment}.

\subsubsection{Acceptable Augmentations}

The possible transformations specified each have a random chance of being applied to each image. It was important to choose a range of possible transformations that would always produce a reasonable augmented image. The permitted augmentations that could be randomly chosen were:

\begin{itemize}
    \item Rotations within a range of $\pm 2$ degrees.
    \item Shifts in the $x$ axis within a range of $\pm 2\%$ of the images' widths.
    \item Shifts in the $y$ axis within a range of $\pm 2\%$ of the images' heights.
    \item Shears within a range of $\pm 2$ degrees.
    \item Zooms in the $x$ and $y$ axes within a range of $\pm 2\%$ of the images' widths or heights.
    \item Brightness shifts within a range of $\pm 10\%$.
    \item Horizontal flips.
    \item Vertical flips.
\end{itemize}

Using larger ranges of permitted transformations resulted in augmented images that did not represent the real data well. Examples of images created using both the ranges defined above and ``unrealistic'' ranges are shown in Figure \ref{fig:augexample}. The shifts in brightness are particularly important as the calibration of the CT machine differs for each scan. As a result, a wide range of brightness distributions exist across different scans. This is discussed in further detail in Section \ref{sec:calcificationimplementation}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/orig-aug.png}
        \caption{Original}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.38\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/bad-aug.png}
        \caption{Unrealistic}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.38\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/good-aug.png}
        \caption{Realistic}
    \end{subfigure}
    \caption{Examples of augmented images created using both ``unrealistic'' and ``realistic'' sets of permitted transformations. The images are shown at the top and their corresponding labels are shown at the bottom. All labels are thresholded after augmentation to ensure that only white or black values are present. \textbf{(a)} The original image-label pair. \textbf{(b)} Two examples of image-label pairs augmented using a set of unrealistic permitted transformations resulting in images that do not represent the real coral data well. The first image's brightness has been shifted too far and the overly aggressive rotation and shifting has resulted in straight labels on the left-most edge which are not realistic. The second image's label has also been rotated too far resulting in a straight label on the right-most edge. \textbf{(c)} Two examples of image-label pairs augmented using the set of permitted transformations used in this project.}
    \label{fig:augexample}
\end{figure}

\subsection{U-Net}

As discussed in Chapter \ref{chap:technical}, the U-Net architecture~\cite{ronneberger2015u} is the main architecture experimented with. The implementation used throughout the project was initially based off of an implementation available on GitHub\footnote{\url{https://github.com/zhixuhao/unet}}, though over the course of the project the majority of the code has been edited. It is written in Python and makes use of the Keras functional API. Recall that the original U-Net architecture consists of three sections: the contracting path, the bottleneck, and the expanding path. A high level diagram of the U-Net architecture is shown in Figure \ref{fig:unetushape} and can be used to better understand the implementation details described.

\subsubsection{The Contracting Path}

The contracting path consists of multiple ``blocks'' with each block consisting of two convolutional layers followed by a 2$\times$2 max-pooling layer with a stride of two. The convolutional layers both use 3$\times$3 kernels and the ReLU activation function. Apart from the first block, the first convolutional layer of each contracting block doubles the number of feature channels. The implementation of the first contracting block is shown in Listing \ref{lst:block1}.

\begin{lstlisting}[float={!t},caption={The implementation of the first contracting block of the U-Net architecture using the Keras functional API.},label={lst:block1},language=Python,upquote=true,belowskip=0pt]
conv1 = Conv2D(64, 3, activation="relu", padding="same")(inputs)
conv1 = Conv2D(64, 3, activation="relu", padding="same")(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
\end{lstlisting}
The \texttt{pool1} tensor defined in Listing \ref{lst:block1} will be the input to the next contracting block, whose output will be the input to the next block, and so on. The first argument of the \texttt{Conv2D} function is the number of output feature channels. Since the number of feature channels doubles at each contracting block, the next contracting block would have two convolutional layers outputting 128 feature channels rather than 64. The second argument is the size of convolutional kernels to use, so 3$\times$3 in this case. Note that the Keras \texttt{MaxPooling2D} function implicitly uses the same stride size as the pool size specified, so a stride of two in the $x$ and $y$ axes will be used.

The \texttt{"{}same"{}} padding argument specifies that the dimensions of the output feature channels should match the dimensions of the inputs. This differs from the implementation of the U-Net architecture used in the original paper. The original U-Net architecture would start with an oversized input. For example, to segment a 388$\times$388 image, a 572$\times$572 image would need to be used as input and the resulting output would have a resolution of 388$\times$388. This decrease in resolution throughout the architecture is a result of no padding being used with each convolutional layer. In order to produce a larger image to be used as input to the network, the original image would be extrapolated via mirroring~\cite{ronneberger2015u}. The use of the \texttt{"{}same"{}} padding allows the output image to be the same resolution as the input image preventing the need for any kind of extrapolation of the 256$\times$256 patches.

\subsubsection{The Bottleneck}

The bottleneck contains only one block which also consists of two convolutional layers each using 3$\times$3 kernels and the ReLU activation function, however no max-pooling layers are present. As a result, the bottleneck block is implemented similarly to the contracting block shown previously, only without the max-pooling layer. At this point, the number of output feature channels has reached 1024. Note that although the original U-Net architecture does not mention any dropout layers, a dropout layer is placed after the last block of the contracting path and another dropout layer is also placed after the bottleneck block. The dropout \texttt{rate} arguments that specify the probability that a neuron is dropped were initially set to 0.5 but this value is experimented with and discussed further in Chapter \ref{chap:evaluation}.

\begin{lstlisting}[float={!t},caption={The implementation of the last expanding block of the U-Net architecture using the Keras functional API.},label={lst:block2},language=Python,upquote=true,belowskip=0pt]
up9 = UpSampling2D(size=(2, 2))(conv8)
up9 = Conv2D(64, 2, activation="relu", padding="same")(up9)
merge9 = concatenate([conv1, up9], axis=3)
conv9 = Conv2D(64, 3, activation="relu", padding="same")(merge9)
conv9 = Conv2D(64, 3, activation="relu", padding="same")(conv9)
\end{lstlisting}
\subsubsection{The Expanding Path}

The expanding path of the architecture described in the original U-Net paper consists of multiple blocks with each block containing a 2$\times$2 ``up-convolution'' (or transposed convolution) layer followed by two convolutional layers each using 3$\times$3 kernels and the ReLU activation function. A 2$\times$2 transposed convolution using a stride of two in the $x$ and $y$ axes results in a doubling of the image size in both axes. The first convolutional layer of each expanding block halves the number of feature channels.

It is in these expanding blocks that the pass-forward of information from the contracting path takes place. The output feature channels from the corresponding blocks in the contracting path are appended to the feature channels that are produced by the transposed convolutions. This new set of feature channels containing both the feature channels from the contracting path, and the feature channels from the transposed convolution are then used as the inputs to the two convolutional layers in each block. As discussed in Chapter \ref{chap:technical}, this allows the features that are learned whilst contracting the image to also be used to reconstruct it. The implementation of the last expanding block is shown in Listing \ref{lst:block2}.

Looking at Listing \ref{lst:block2} it can be seen that the 2$\times$2 transposed convolution used in the original U-Net architecture is replaced with a 2$\times$2 upsampling layer (with an implicit 2$\times$2 stride) followed by a regular 2$\times$2 convolution. The original U-Net paper also does not mention an activation function being applied to the output of the transposed convolutions, whereas a ReLU activation is used here. These architectural design choices stem from the initial implementation used and although these do differ from the original architecture, the effects on the performance achieved are negligible. Note that the output feature channels of the \texttt{up9} layer are appended to the feature channels of the \texttt{conv1} layer from the first contracting block using the \texttt{concatenate} function.

\begin{figure}[!t]
    \centering
    \hspace*{-0.3cm}
    \input{tikz/unet}
    \caption[A diagram of the U-Net architecture (created using an open-source CNN architecture visualisation tool). The architecture is named after the U-shape that it resembles when illustrated in this way. Each cube represents a layer with the number of output feature maps shown at the bottom of the cube. At the bottleneck there are 1024 feature maps that are each one sixteenth the size of the input image in the $x$ and $y$ dimensions. The green arrows represent the feedforward of information from one layer to the next, whereas the blue arrows represent the concatenation of the feature maps of one layer to another. Note that the max-pooling layers at the end of each contracting block are shown at the start of the next block for easier illustration of the pass forward of the feature channels. Similarly, the up-convolution layers are shown at the end of each contracting block rather than at the start of the next.]{A diagram of the U-Net architecture (created using an open-source CNN architecture visualisation tool\footnotemark). The architecture is named after the U-shape that it resembles when illustrated in this way. Each cube represents a layer with the number of output feature maps shown at the bottom of the cube. At the bottleneck there are 1024 feature maps that are each one sixteenth the size of the input image in the $x$ and $y$ dimensions. The green arrows represent the feedforward of information from one layer to the next, whereas the blue arrows represent the concatenation of the feature maps of one layer to another. Note that the max-pooling layers at the end of each contracting block are shown at the start of the next block for easier illustration of the pass forward of the feature channels. Similarly, the up-convolution layers are shown at the end of each contracting block rather than at the start of the next.}
    \label{fig:unetushape}
\end{figure}

\subsubsection{Output}

\footnotetext{\url{https://github.com/HarisIqbal88/PlotNeuralNet}\label{plotNN}}

After the last expanding block, a final 1$\times$1 convolutional layer is present in the original U-Net architecture. This convolutional layer has two output feature channels, one corresponding to each possible class that a pixel could be classified as. Although the original paper does not mention an activation function of any kind, the sigmoid activation function is used in this implementation ensuring that the output values represent the probability that a given pixel is ``part of a boundary''. As mentioned earlier, a high level diagram of the U-Net architecture is shown in Figure \ref{fig:unetushape} and can be used to better understand the implementation details described.

\subsubsection{The Keras \texttt{Model} Class}

Finally, the model is stored in an instance of the Keras \texttt{Model} class. Both the input and output tensors are passed as arguments to the \texttt{Model} constructor and the resulting object contains all of the layers and their connections as attributes. Once an instance of the \texttt{Model} class has been created, the model can be configured using the \texttt{compile} method, trained with the \texttt{fit} method, or used for inference with the \texttt{predict} method.

\subsection{Training}

Once the architecture and online data augmentation process had been implemented, the model was then compiled using the \texttt{compile} method from the \texttt{Model} class. The \texttt{compile} method can then be used to define which optimiser and loss function to use, and which metrics to monitor, and the \texttt{fit} method can be used to train the network.

\subsubsection{Keras Callbacks}

The Keras callbacks are objects that can perform actions at various stages of training (e.g., at the start or end of epochs or batches)\footnote{\url{https://keras.io/api/callbacks}}. Callbacks were used to implement early stopping, model checkpointing, and metric visualisation using TensorBoard\footnote{\url{https://www.tensorflow.org/tensorboard}}.

As discussed in Chapter \ref{chap:technical}, early stopping is a regularization technique in which the training process is stopped given some ``stopping criteria'' rather than after a set number of epochs. The Keras library allows users to implement early stopping using the \texttt{EarlyStopping} callback. Both the metric to monitor and a \texttt{patience} argument must be provided to the \texttt{EarlyStopping} callback. In this case, the loss achieved on the validation set is monitored. The \texttt{patience} argument defines the number of epochs that the validation loss can increase before the training is stopped.

Model checkpointing and TensorBoard visualisation are implemented with the \texttt{ModelCheckpoint} and \texttt{TensorBoard} callbacks respectively. The \texttt{ModelCheckpoint} callback simply saves the parameters of the entire network after each epoch to a \texttt{.hdf5} file. Using the \texttt{save\_best\_only=False} argument and a filename formatted using the current epoch number ensures that each epoch is saved as a new file, rather than the previous epoch's save being overwritten. Example usage of the callbacks discussed are shown in Listing \ref{lst:compile}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/initial-results.png}
    \caption{Initial results achieved by the network. Note that the hyperparameters chosen are not necessarily optimal and that performance may still be improved. \textbf{(top)} The input samples from the test set. \textbf{(centre-top)} The ground truth labels. \textbf{(centre-bottom)} The predictions output by the network. \textbf{(bottom)} The ground truth labels (in green) overlaid with the predictions.}
    \label{fig:outputs}
\end{figure}

\begin{table}[t]
    \centering
    \caption{A summary of the initial hyperparameter settings used.}
    \begin{tabular}{@{}ll@{}}
    \toprule
    Hyperparameter   & Setting      \\ \midrule
    Architecture     & 2D U-Net   \\
    Optimisation Algorithm & Adam \\
    Learning rate & 0.0001 \\
    Loss function & Cross-entropy \\
    Epochs & 20 \\
    Steps per epoch & 500 \\
    Batch size & 2 \\ \bottomrule
    \end{tabular}
    \label{tab:initialhyperparams}
\end{table}

\subsubsection{Initial Results}

A few examples of the initial results achieved are shown in Figure \ref{fig:outputs}. These initial results were output by a network trained using the Adam optimiser~\cite{adam} with a learning rate of 0.0001 to optimise the binary cross-entropy loss function. The network was trained for 20 epochs with each epoch consisting of 500 augmented batches. A batch size of two was used so the total of $20\times500\times2=\text{20,000}$ augmented samples were seen by the network. These hyperparameters are summarised in Table \ref{tab:initialhyperparams}. An average per-pixel accuracy of 96.7\% was acheived. Note that this high accuracy is misleading and is discussed in Section \ref{sec:accimplementation}. The entire training process took ${\sim}$16 minutes on an Nvidia P100 ``Pascal'' GPU. Many more examples of boundary predictions resulting from networks trained with different hyperparameters are shown throughout Chapter \ref{chap:evaluation}. Listing \ref{lst:compile} shows a simplified example of the compilation of a network configured with these hyperparameters.

\subsection{Version Control}

The Git version control system was used to backup and keep track of the various iterations of the code implemented throughout the project. The code for all of the network implementations and the scripts used are available on GitHub\footnote{\url{https://FILLTHISIN}}\footnote{\url{https://FILLTHISIN2}} (FILL THIS IN). 

\section{Three Dimensional Boundary Extraction}
\label{sec:threedimension}

This section discusses the attempts to implement and train a modified U-Net architecture capable of extracting the annual density banding present in three dimensional data. An overview of the components implemented in this section is shown in Figure \ref{fig:3doverview}. As opposed to only utilising the density information available in a single slice, the modified architecture also makes use of the density information in surrounding slices when determining boundary positions. Due to the corallite structures present in the coral data, an architecture that could make use of a third dimension could achieve better performance. The corallite structures are the thin bright lines visible in the coral images that are perpendicular to the growth surface of the coral. It can be seen in Figure \ref{fig:outputs} that the corallite structures are sometimes even predicted as density band boundaries by the two dimensional architecture. Whilst corallite structures vary significantly in adjacent slices due to their small size (often less than a millimetre in diameter), the annual density banding is consistent across tens of slices. An architecture that could make use of these commonalities across slices could thus perform significantly better.

\subsection{Dataset Expansion}

In order to train the modified architecture, the curated dataset discussed in Section \ref{sec:datasetcuration} was expanded to include data from slices surrounding the slices initially labelled. Since the modified architecture makes use of up to nine slices (the central slice and four slices on either side), the eight slices surrounding the labelled slices also needed to be extracted. As mentioned in Section \ref{sec:2dlabel}, when the sliding window Python script was used to create the 2D dataset, patches from surrounding slices were also produced. The resulting 3D dataset contains the same amount of samples (388) with each sample consisting of nine patches and the label consisting of the central labelled patch. The nine-sample patches are stored with the same name as the label apart from a suffix of \texttt{\_0} to \texttt{\_8} denoting the patches' positions relative to the central patch which has a suffix of \texttt{\_4}.

\subsection{Data Loader Implementation}

Whereas the 2D architecture made use of the \texttt{ImageDataGenerator} class to load 2D images into the model, a 3D version of the \texttt{ImageDataGenerator} class is not currently implemented in the Keras library. Although the \texttt{ImageDataGenerator} class could be used to simply load nine patches one after another and the Keras \texttt{stack} method could be used to stack the images to produce a $(9 \times y \times x \times 1)$ tensor, this method would not allow for online augmentation to be performed since different random transformations would be applied to all nine of the loaded patches. Note also that even if the same transformations were able to be applied to each 2D patch, transformations in the third dimension would not be possible. Thus, a custom data loader was written.

An \texttt{ImageDataGenerator3D} class and a \texttt{LabelDataGenerator2D} class were implemented. These were designed to be similar to the Keras \texttt{ImageDataGenerator} class allowing the same data augmentation arguments to be passed to their constructors. Recall that in order to produce 2D image-label pairs, a pair of generators created using the \texttt{flow\_from\_directory} method were used with the same \texttt{seed} to augment the image-label pairs with the same transformations. The online augmentation of the 3D data is designed to follow a similar structure; a pair of generators created using the \texttt{flow\_from\_directory} method of the \texttt{ImageDataGenerator3D} and \texttt{LabelDataGenerator2D} classes with the same \texttt{seed} argument are used to perform online augmentation to both the nine-patch sample and the single label patch. The \texttt{ImageDataGenerator3D} class is used to load and augment the 3D samples and the \texttt{LabelDataGenerator2D} class is used to load and augment the 2D labels.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{images/3D-overview.pdf}
    \caption{An overview of the components implemented in this section. Each training sample that the modified architecture trains on will consist of nine patches. Only the centre slice is labelled and the model attempts to extract the boundaries present in this central slice. A custom 3D ``data loader'' is implemented in order to perform online augmentation of the nine-patch training samples.}
    \label{fig:3doverview}
\end{figure}

\subsubsection{The Custom \texttt{flow\_from\_directory} Methods}

The \texttt{flow\_from\_directory} method of the \texttt{ImageDataGenerator3D} class first sorts the \texttt{.png} images from the sample directory alphanumerically. Nine images are then loaded, stacked, and ``unsqueezed'' resulting in a $(9 \times y \times x \times 1)$ tensor. The unsqueeze operation takes for example a $(y \times x)$ tensor and produces a $(y \times x \times 1)$ tensor. These stacks of nine images are then augmented using techniques discussed later in this section and are finally stacked into $(batch \times 9 \times y \times x \times 1)$ tensors where $batch$ is the batch size.

The \texttt{flow\_from\_directory} method of the \texttt{LabelDataGenerator2D} class also sorts the \texttt{.png} images from the label directory alphanumerically. These images are loaded and unsqueezed resulting in a $(y \times x \times 1)$ tensors containing a single label. The appropriate augmentation transformations are then applied and the resulting tensors are stacked into $(batch \times y \times x \times 1)$ tensors.

The two generators produced by the \texttt{flow\_from\_directory} methods of the \texttt{ImageDataGenerator3D} and \texttt{LabelDataGenerator2D} classes were then zipped together to create a single generator that yielded sample-label pairs with each sample consisting of nine patches, and each label consisting of only a central labelled patch. Similarly to the Keras \texttt{flow\_from\_directory} methods used when loading and augmenting the 2D data, these custom methods could also accept a \texttt{seed} argument ensuring that the same transformations are applied to the sample-label pairs. The custom \texttt{flow\_from\_directory} method was also capable of shuffling the order in which samples and labels were loaded and \texttt{seed} argument ensured that the samples and labels were shuffled in the same order.

Example usage of the \texttt{ImageDataGenerator3D} and \texttt{LabelDataGenerator2D} classes in order to perform online 3D augmentation is shown in Listing \ref{lst:3Daugment}.

\subsubsection{Three Dimensional Transformations}
\label{sec:3daug}

The custom data loader enabled the following transformations:
\begin{itemize}
    \item Random rotations in the $x$, $y$, and $z$ dimensions within a range of $\pm 2$ degrees.
    \item Random flips in the $x$, $y$, and $z$ dimensions.
    \item Random brightness shifts within a range of $\pm 10$\%.
\end{itemize}
Although these are the ranges used to train this particular model, these transformations can use values within any given range, not just the ranges specified above.

% Shifts in the $x$, $y$, and $z$ axes were also possible but increased training times significantly (taking up to X times as long) and so were not used when training. (maybe say here how long loading the data and augmenting took??)

\subsection{Architecture Modification}

Although various modified architectures were experimented with and are discussed in Chapter \ref{chap:evaluation}, only the initial modified architecture experimented with will be described here.

At a high level, the modification consisted of replacing all of the 2D operations in the contracting path with their 3D counterparts. However, issues arise when a tensor output from a 3D layer is passed to a 2D layer. Whereas the Keras \texttt{Conv3D} layer requires an input tensor with shape ($batch \times z \times y \times x \times channels$) and produces a tensor with the same shape, the \texttt{Conv2D} layer inputs and outputs tensors with the shape ($batch \times y \times x \times channels$). Thus, in order to pass the resulting tensor to a 2D layer, the tensor of shape ($batch \times z \times y \times x \times channels$) must be reshaped into a ($batch \times y \times x \times (z \times channels)$) tensor. A diagram of the modified U-Net architecture is shown in Figure \ref{fig:unetushape3D}.

\begin{lstlisting}[float={!t},caption={The implementation of the first contracting block of the modified U-Net architecture using the Keras functional API.},label={lst:3Dblock},language=Python,upquote=true,belowskip=0pt]
conv1 = Conv3D(8, 3, activation="relu", padding="same")(inputs)
conv1 = Conv3D(8, 3, activation="relu", padding="same")(conv1)
pool1 = MaxPooling3D(pool_size=(1, 2, 2))(conv1)
conv1 = Reshape((256, 256, 72))(conv1)
\end{lstlisting}
The implementation of the first block of the modified contracting path is shown in Listing \ref{lst:3Dblock}. It can be seen that the two 2D 3$\times$3 convolutional layers have been replaced with 3D convolutional layers using 3$\times$3$\times$3 kernels (the kernel size is defined by the second argument of the \texttt{Conv3D} method). It can also be seen that although the max-pooling layer still uses a pool size and stride of two in the $x$ and $y$ axes, a value of one is instead used in the $z$ axis. A pool size and stride of two cannot be used in the $z$ axis as the input tensors only have a $z$ dimension of nine. Since there are four max-pooling operations, the tensor would be reduced to a size of just one in the $z$ dimension, assuming that padding was used at each stage to pad the $z$ dimension to an even number.

After the convolutional and max-pooling layers, the tensor is then reshaped to a ($batch \times 256 \times 256 \times 72$) tensor allowing it to be concatenated to the feature channels of the 2D layers present in the expanding path. Note that there are only eight output feature channels. Since the number of output feature channels will be multiplied by nine, a number of feature channels was chosen in order to make the reshaped tensor as close to the ($batch \times 256 \times 256 \times 64$) tensor that was originally used in the 2D architecture as possible. Various numbers of output channels were experimented with and are discussed in Chapter \ref{chap:evaluation}.

The remaining contracting blocks are implemented similarly to the first block shown in Listing \ref{lst:3Dblock}. However, the tensor resulting from the last block of the contracting path must be reshaped before it is passed to the bottleneck since 2D convolutional layers are still used in the bottleneck block. The remainder of the network is implemented as it was in the 2D architecture.

\begin{figure}[!t]
    \centering
    \hspace*{-0.3cm}
    \input{tikz/unet3D}
    \caption[A diagram of the modified U-Net architecture (created using an open-source CNN architecture visualisation tool). Each cube represents a layer with the number of output feature maps shown at the bottom of the cube. The green arrows represent the feedforward of information from one layer to the next, whereas the blue arrows represent the concatenation of the feature maps of one layer to another.]{A diagram of the modified U-Net architecture (created using an open-source CNN architecture visualisation tool\textsuperscript{\ref{plotNN}}). Each cube represents a layer with the number of output feature maps shown at the bottom of the cube. The green arrows represent the feedforward of information from one layer to the next, whereas the blue arrows represent the concatenation of the feature maps of one layer to another. Note that the max-pooling layers on the contracting blocks are three dimensional 1$\times$2$\times$2 max-pooling layers whereas the bottleneck max-pooling layer is simply a two dimensional 2$\times$2 max-pooling layer.}
    \label{fig:unetushape3D}
\end{figure}

% Although the nine-patch sample could simply be input into the 2D architecture as a 2D images with nine channels allowing the network to make use of nine patches as opposed to one, an input tensor of this shape would not enable 3D convolutions. Without the use of 3D convolutions, the network can not make 

% The modified architecture does not perform as well as the 2D architecture. Inspecting the results both ... are shown in Chapter \ref{chap:evaluation}.

\subsection{Fully Three Dimensional Architecture}

A fully 3D U-Net architecture was also implemented and experimented with. This network takes nine-patch labels rather than a single central label and attempts to predict the boundaries present in all nine sample patches. In this case, the architecture modification consisted of replacing all of the 2D operations present in the network with their 3D counterparts. A pool size and stride of one was again used in the $z$ axis of the max-pooling layers.

In order to train this fully 3D architecture, a new dataset containing nine-patch samples and nine-patch labels was curated using the same sliding window technique. Since only a small selection of slices were initially labelled to curate the original 2D dataset, a larger selection of slices adjacent to the initially selected slices were labelled manually. Ultimately, labelling enough data for this model to perform well proved challenging---curating a dataset with the same amount of samples used to train the 2D architecture would require nine times the amount of labelled slices.

Both the experimentation with this architecture and the challenges involved in labelling 3D data are discussed further in Chapter \ref{chap:evaluation}.

\subsection{Initial Results}

Examples of the initial results achieved using the modified architecture that outputs a single prediction of the boundaries present in the central slice are shown in Figure \ref{fig:outputs2}. These initial results were output by a network trained using the Adam optimiser with a learning rate of 0.0001 to optimise the binary cross-entropy loss function. The network was trained for 20 epochs with each epoch consisting of 500 augmented samples. A batch size of two was used so the total of $20\times500\times2=\text{20,000}$ augmented nine-patch samples were seen by the network. The entire training process took ${\sim}$50 minutes on an Nvidia P100 ``Pascal'' GPU.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/initial-results-3D.png}
    \caption{Initial results achieved by the modified network that predicts a central label only. Note that the hyperparameters chosen are not necessarily optimal and that performance may still be improved. \textbf{(top)} The input samples from the test set. \textbf{(centre-top)} The ground truth labels. \textbf{(centre-bottom)} The predictions output by the network. \textbf{(bottom)} The ground truth labels (in green) overlaid with the predictions.}
    \label{fig:outputs2}
\end{figure}

\section{Accuracy Metric Implementation}
\label{sec:accimplementation}

As discussed briefly in Chapter \ref{chap:context}, choosing an appropriate accuracy metric for the boundary extraction task was particularly challenging. This section discusses the need for a custom accuracy metric and then details its implementation.

\subsection{Motivation}

There are two major issues that arise when using a standard per-pixel accuracy metric to quantify the performance achieved in this task. The first issue stems from the severe class imbalance present in the labels. On average, ${\sim}$98\% of a labelled patch is black---part of the ``not part of a boundary'' class. As a result, a model that learned to output a pure black image would achieve an average accuracy of 98\%. The inherent class imbalance gives rise to another problem when assessing the performance of the model. Since the boundary labels are only one pixel wide, a model that predicts boundary positions that are just one pixel off of the correct positions could potentially achieve an accuracy of 0\%. A custom accuracy metric must be conceived to ensure that a model that predicts boundaries a few pixels away from the manually labelled positions is still rewarded. Eventually, a metric based off of the average Euclidean distances between the predicted and ground truth boundaries was designed.

\subsection{Methodology}

There are multiple steps involved in the calculation of the custom accuracy. First, the prediction output by the model is thresholded using the Otsu method~\cite{otsu}. The Otsu method finds the threshold value that minimises the weighted within-class variance. Once this threshold value $t$ is found, all pixels with values less than the threshold value are set to 0 and all pixels with values greater than $t$ are set to 255. This thresholded image can then be ``skeltonized''. Skeltonization is the process of reducing the foreground regions of binary images to just one pixel wide representations. The scikit\footnote{\url{https://scikit-image.org}} implementation of Zhang's method~\cite{zhang} is used to skeletonize the image. Introduced in 1984, Zhang's method iteratively removes pixels present on the foreground region borders until no more pixels can be removed. An example of the skeletonization of a prediction using this method is shown in Figure \ref{fig:skeletonisation}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/skel-pred.png}
        \caption{Original prediction}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/skel-thresh.png}
        \caption{Thresholded prediction}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=1\textwidth, valign=c]{images/skel-skel.png}
        \caption{Skeletonized prediction}
    \end{subfigure}
    \caption{An example of the skeletonization of a prediction output by the 2D architecture. The image is first thresholded using the Otsu method and then skeletonized using Zhang's method. Similarly to the ground truth labels, the resulting image contains shapes that are only one pixel wide.}
    \label{fig:skeletonisation}
\end{figure}

\subsubsection{Average Distance Calculation}

Once the prediction is skeletonized, the ``average distance'' from the prediction to the ground truth label can be calculated. For each white pixel in the skeletonized prediction, the closest white pixel in the label is found. The closest white pixel has the shortest Euclidean distance between itself and the prediction's white pixel. These distances are calculated as if the images were overlaid with each other. For example, a white pixel with position (24, 36) in the skeletonized prediction would have a Euclidean distance of zero from a white pixel also with position (24, 36) in the ground truth label.

Once these closest distances have been found for each pixel, they are averaged to calculate the average distance from the prediction to the label. This process is then repeated in order to calculate the average distance from the label to the prediction. These two distances are then averaged once again to calculate a final average distance between the two images. Note that the average distance from the prediction to the label can be significantly different from the average distance from the label to the prediction, and so averaging the two is important.

Since the metric relies on the average distance between white pixels, issues arise when a pure black image is predicted by the network. If a prediction contains no white pixels, it is not clear what the distance between the label and the prediction should be. A distance of $\infty$ would not be able to contribute to an accuracy averaged over all samples. It is important for a prediction of an all black image to achieve a low accuracy metric; removing these from the average accuracy achieved would result in the average being positively skewed. Thus, a single white pixel is placed in the centre of a pure black prediction, ensuring that all samples receive a finite accuracy and contribute to the overall average.

% For example, imagine a label with five boundaries present and a prediction that has only predicted one of the five boundaries but has predicted this boundary well. The average distance from the prediction to the label would be very low as for each white pixel in the one predicted boundary, there are white pixels very close by in the ground truth label. However, the average distance from the label to the prediction would be significantly higher; for each white pixel in the remaining four boundaries, the closest white pixels would be

\subsubsection{Accuracy Calculation}

The average distance between the prediction and the label can be thought of as an error metric rather than an accuracy metric; the lower the distance, the better the model has performed in producing that prediction. Accuracy metrics are often represented as a percentage where a higher percentage value corresponds to better performance. In order to represent this accuracy metric as a percentage, the accuracy achieved is calculated as

\begin{equation*}
    \frac{d_{max} - {d}}{d_{max}} \times 100
\end{equation*}

where $d_{max}$ is the maximum average distance that two images could be, and $d$ is the actual distance calculated. $d_{max}$ would theoretically be achieved if the prediction contained a single white pixel in one of the corners, and the ground truth label contained a single white pixel in the opposite corner. This would result in a maximum theoretical distance of 362 for 256$\times$256 patches. However, since the label patches present in the curated dataset must contain labelling across the entire patch, this scenario can never occur. The maximum distance achievable on a single label in the dataset was actually 131 pixels and the average across the entire dataset was ${\sim}110$ pixels. Ultimately, a maximum of 90 pixels was chosen, with errors above 90 pixels being truncated to an accuracy of 0\%. This lower maximum (in comparison with the theoretical maximum of 362) was chosen to ensure that accuracy percentages over 90\% correspond to predictions that are often considered acceptable when inspected visually.

The initial two dimensional U-Net implementation achieved a validaiton accuracy of ${\sim}94\%$ and the initial three dimensional implementation achieved a validation accuracy of ${\sim}$77\%. The accuracy metric and its various strengths and weaknesses are discussed further in Chapter \ref{chap:evaluation}.

\subsection{The \texttt{ctypes} Module}

Since the entirety of the project is written in Python, the accuracy metric calculation was also written in Python, allowing it to be easily integrated into the training process. However, when written in Python, the calculation of the custom accuracy metric for a single sample took an average of 38 seconds. Calculating the average accuracy achieved across the entire training set would therefore take up to three hours. Since the distances from a white pixel in one image to all white pixels in the other must be calculated in order to find the minimum, the time complexity of the algorithm is $O(n^2)$ and this computation is unavoidable.

In order to reduce the time taken to calculate the accuracy metric, an implementation was written in C and the \texttt{ctypes} module was used to wrap the C implementation in Python. The C code is compiled into a \texttt{.so} library. Using the \texttt{ctypes} module, this library can be loaded and its methods can be accessed in Python. 

Once implemented and wrapped in a Python function, the C implementation took an average of 0.12 seconds to calculate the accuracy metric for a single sample\footnote{The times achieved by the Python and C implementations are taken when using an Intel Core i5 9400 processor.}. This provides a ${\sim}315$ times speed up compared to the pure Python implementation, and allows for the accuracy across the entire training set to be calculated in under a minute.

\section{Calcification Rate Estimation}
\label{sec:calcificationimplementation}

As outlined in Chapter \ref{chap:context}, the objective of this project is to utilise the implemented networks to estimate the calcification rate---the mass of skeletal matter produced annually. In order to estimate the calcification rate, two values are required: the linear extension rate (the distance that the coral grows per year) and the density of the matter being produced. The calcification rate can then be calculated by multiplying these two values.

\subsection{Linear Extension Rate Estimation}

In order to estimate the linear extension rate, the average distance grown annually in terms of voxels must first be calculated. Since an idealised annual density cycle is in the form of a sinusoidal wave with the density gradually changing from high to low and back over the course of a year~\cite[p. 39]{coralsine}, the distance grown in a year is actually the distance between every second boundary (see Figure \ref{fig:calcification}).

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.89]
            \node [anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=1\textwidth]{images/smooth-label.png}};
            \draw [<->, red, thick] (4.5,4.6) -- (5.1,3.9);
            \draw [<->, red, thick] (2.7,2.95) -- (3.4,1.85);
        \end{tikzpicture}
        \caption{}
        \label{fig:calcification}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \begin{tikzpicture}[scale=0.89]
            \node [anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=1\textwidth]{images/calcification.png}};
            \node[white] at (6.4, 2.8) {\footnotesize{1}};
            \node[white] at (5.9, 3.3) {\footnotesize{2}};
            \node[white] at (5.45, 3.8) {\footnotesize{3}};
            \node[white] at (5.05, 4.21) {\footnotesize{4}};
            \node[white] at (4.7, 4.6) {\footnotesize{5}};
            \node[white] at (4.35, 4.97) {\footnotesize{6}};
        \end{tikzpicture}
        \caption{}
        \label{fig:calcificationrect}
    \end{subfigure}
    \caption{\textbf{(a)} Examples of the linear extension rate. The distance grown in a year is the distance between every second boundary. The bottom arrow starts at the end of a high density band and finishes at the end of the next high density band, whereas the top arrow starts at the end of a low density band and finishes at the end of the next low density band. It can be seen that even though both of the arrows shown are valid measurements of the linear extension rate, the distances measured at different points in the skeleton can vary significantly. \textbf{(b)} An illustration of the rectangular area created given two points: one inside and one outside the skeleton. CCA is then used to uniquely label the boundaries from the outside in.}
\end{figure}

A Python script was written that takes an image and two sets of coordinates as arguments. The image is then split into overlapping 256$\times$256 patches using a sliding window technique similar to that utilised in Section \ref{sec:datasetcuration}. The boundaries present in these patches are then predicted by the network and the boundaries present in the entire image are then reconstructed. During this reconstruction, only the inner areas of the predictions are taken into account since the predictions made at the edges of patches are often of a lower quality (due to the lack of context available). For example, looking at Figure \ref{fig:skeletonisation}, it can be seen that the outermost pixels of the boundaries are more blurry and tend to be less confidently predicted by the network. Once the boundaries across the entire image have been predicted, they are then skeletonized.

The two supplied sets of coordinates should correspond to two points on the image. The first should be a point outside the growth surface of the coral, and the second a point inside the coral skeleton. When a line is drawn between these supplied points, the line should be parallel to the direction of growth of the coral, as this ensures that the distances measured between the boundaries are more accurate. Using these two supplied points, a rectangular area is created as shown in Figure \ref{fig:calcificationrect}. This area is not only used to determine the linear extension rate but also to calculate the average density of the skeletal matter later on.

The OpenCV \texttt{connectedComponents} method is then used to perform connected-component analysis (CCA). This method determines the positions of the individual skeletonized boundaries present in the rectangular area and supplies each boundary with a unique label. Once these unique labels are determined, the boundaries are sorted by the order in which they occur relative to the outside of the skeleton (e.g., the growth surface first, followed by the outermost boundary, and so on). The distances between these boundaries are then calculated using an average of the minimum Euclidean distances from one boundary to the next (i.e., for each pixel in one boundary, the closest pixel in the next boundary is found, and the distance from the first boundary to the next is the average of these closest distances for each pixel). 

Since the distance grown annually is the distance between every second boundary, the number of pixels grown per year can be estimated by multiplying the average distance between the boundaries by two. The resulting value represents the linear extension rate in terms of pixels per year. In order to convert this value to millimetres per year, it must be multiplied by the pixel size. The pixel size is the real distance that each pixel represents and is constant across all three dimensions (i.e., a voxel represents the same distance in the $x$ axis as it does in the $z$ axis). A common pixel size for the scans composing this dataset is ${\sim}0.1$ mm. Once the linear extension rate has been multiplied by this pixel size, the linear extension rate in terms of millimetres per year (mm y$^{-1}$) has been calculated.

\subsection{Density Estimation}

In order to calculate the density that each pixel value corresponds to, a calibration curve must be fit. Given the values of five densities that are known to correspond to five greyscale values, the non-linear relationship between a pixel's density and its corresponding greyscale value can be determined. This calibration curve is fit using \texttt{curve\_fit}\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html}}, the SciPy implementation of the non-linear least squares curve fitting method. In this case, a quadratic function is fit to the data. Once this function that maps a pixel's greyscale value to a corresponding density is found, it can be used to estimate each pixel's density in g cm$^{-3}$.

When estimating the average density of the skeletal matter produced, only pixels that lie within the rectangular area (seen in Figure \ref{fig:calcificationrect}) are taken into account. This ensures that the same skeletal matter used to estimate the linear extension rate is used to estimate the average density. However, since this rectangular area contains both pixels that are within the skeleton and pixels that are not, two mean density values will exist. In order to find the mean that corresponds to the pixels that are within the coral skeleton, the Scikit-learn implementation of the K-means algorithm~\cite{kmeans} is used\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html}}. It is assumed that only two means should be present---one corresponding to the skeletal pixels, and one corresponding to the other pixels---so the \texttt{n\_clusters} argument is set to two. It is also assumed that the mean corresponding to the pixels within the coral skeleton should be higher than the mean corresponding to the pixels outside since coral skeletons are denser than air. Thus, the maximum of the two means discovered is taken to be the average density value of the skeletal matter that lies within the rectangular area.

With the linear extension rate (cm y$^{-1}$) and the density (g cm$^{-3}$) estimated, these two values can now simply be multiplied together to calculate the calcification rate (g cm$^{-2}$ y$^{-1}$). The calcification rates estimated for various scans are discussed in Chapter \ref{chap:evaluation}.