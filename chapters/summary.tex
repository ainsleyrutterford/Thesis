% {\bf A compulsory section, of at most $1$ page} 
% \vspace{1cm} 

% \noindent
% This section should pr\'{e}cis the project context, aims and objectives,
% and main contributions (e.g., deliverables) and achievements; the same 
% section may be called an abstract elsewhere.  The goal is to ensure the 
% reader is clear about what the topic is, what you have done within this 
% topic, {\em and} what your view of the outcome is.

% The former aspects should be guided by your specification: essentially 
% this section is a (very) short version of what is typically the first 
% chapter.  Note that for research-type projects, this {\bf must} include 
% a clear research hypothesis.  This will obviously differ significantly
% for each project, but an example might be as follows:

% \begin{quote}
% My research hypothesis is that a suitable genetic algorithm will yield
% more accurate results (when applied to the standard ACME data set) than 
% the algorithm proposed by Jones and Smith, while also executing in less
% time.
% \end{quote}

% \noindent
% The latter aspects should (ideally) be presented as a concise, factual 
% bullet point list.  Again the points will differ for each project, but 
% an might be as follows:

% \begin{quote}
% \noindent
% \begin{itemize}
% \item I spent $120$ hours collecting material on and learning about the 
%       Java garbage-collection sub-system. 
% \item I wrote a total of $5000$ lines of source code, comprising a Linux 
%       device driver for a robot (in C) and a GUI (in Java) that is 
%       used to control it.
% \item I designed a new algorithm for computing the non-linear mapping 
%       from A-space to B-space using a genetic algorithm, see page $17$.
% \item I implemented a version of the algorithm proposed by Jones and 
%       Smith in [6], see page $12$, corrected a mistake in it, and 
%       compared the results with several alternatives.
% \end{itemize}
% \end{quote}

This project uses deep learning to automate the estimation of the amount of skeletal matter produced annually by corals. In particular, the labelling of density bands present in two dimensional slices of coral is automated. This is achieved using a convolutional neural network based off of the U-Net architecture. To train this network, a dataset containing ${\sim}400$ images is curated using slices extracted from four different three dimensional CT scans of coral skeletons. Various data augmentation techniques are used to artificially expand the dataset and improve the performance achieved. In order to effectively measure performance, an accuracy metric based off the average Euclidean distance between predicted and ground truth boundaries is conceived. Various ablation studies and rigorous quantitative and qualitative evaluation are applied to pinpoint performance and limitations. An ablated version of the U-Net architecture was found to perform better in this task and ultimately achieved a cross-validated accuracy of 77.8\%.

The automatic labelling of the banding present in three dimensions is also explored. To this end, the U-Net architecture is modified to process three dimensional data, and a custom three dimensional data loader capable of online augmentation is implemented. In order to train this network, a larger dataset of manually labelled adjacent slices is curated, containing over 3,400 images extracted from four coral samples.

The densities of the coral skeletons, and the distances between the density bands extracted by the network are automatically estimated using various classical image processing techniques. These estimates are used to calculate the amount of skeletal matter produced annually. The final calculations are similar to the manual calculations reported in the literature.

\section*{Summary of Achievements}

\begin{itemize}
    \item I spent 25 hours manually labelling coral data.
    \item I generated a 2D dataset containing ${\sim}400$ images and a 3D dataset containing ${\sim}$3,400 images using various scripts I wrote.
    \item Having not taken the ``Deep Learning'' unit, I spent 20 hours researching and learning about the field of deep learning.
    \item I learned how to use the Keras and TensorFlow libraries in order to implement the networks used.
    \item I modified an existing 2D U-Net implementation and trained it on the curated dataset.
    \item I extended the U-Net architecture to process 3D data.
    \item I wrote a ``data generator'' from scratch in Python to allow ``online'' augmentation of 3D data as it is passed into the network since Keras does not have its own implementation.
    \item I implemented a custom accuracy metric in C and wrapped it in a Python function to achieve a ${\sim}315$ times speedup allowing the accuracy to be calculated in minutes rather than hours.
    \item I trained the network over 100 times whilst performing hyperparameter optimisation, in order to gain a better understanding of the model and to maximise the performance achieved.
    \item I successfully used ablation studies to find a simplified architecture that performs better than the original U-Net architecture at labelling the boundaries present in two dimensions.
    \item I cross-validated the results achieved and compared them to results achieved by other architectures.
\end{itemize}