% {\bf A compulsory chapter,     of roughly $10$ pages} 
% \vspace{1cm} 

% \noindent
% This chapter is intended to describe the technical basis on which execution
% of the project depends.  The goal is to provide a detailed explanation of
% the specific problem at hand, and existing work that is relevant (e.g., an
% existing algorithm that you use, alternative solutions proposed, supporting
% technologies).  

% Per the same advice in the handbook, note there is a subtly difference from
% this and a full-blown literature review (or survey).  The latter might try
% to capture and organise (e.g., categorise somehow) {\em all} related work,
% potentially offering meta-analysis, whereas here the goal is simple to
% ensure the dissertation is self-contained.  Put another way, after reading 
% this chapter a non-expert reader should have obtained enough background to 
% understand what {\em you} have done (by reading subsequent sections), then 
% accurately assess your work.  You might view an additional goal as giving 
% the reader confidence that you are able to absorb, understand and clearly 
% communicate highly technical material.

As outlined in Chapter \ref{chap:context}, the objective of this project is to use deep learning to automate the estimation of the amount of skeletal matter produced annually by corals. This chapter introduces and briefly describes the techniques used in order to achieve this goal.

\section{Deep Learning}
\label{sec:deeplearning}

Based loosely on the structure of the brain, artificial neural networks (ANNs) are computational models that have proved useful in a wide range of applications~\cite{lecun2015deep, healthcare, nlp}---a notable example being the recent success of convolutional neural networks in the field of computer vision~\cite{compvision, semanticsegreview}. Deep learning is a form of machine learning that concerns the use of ANNs with many layers---hence the name ``deep'' learning. The field has seen a significant increase in popularity in recent years since a network named AlexNet famously won the ImageNet Large Scale Visual Recognition Challenge in 2012\footnote{\url{http://www.image-net.org/challenges/LSVRC/2012/results.html}}, performing considerably better than the previous state-of-the-art~\cite{alexnet}.

A typical fully connected ANN is a layered network of ``neurons'' connected by a series of ``weights''. A neuron is simply an object that produces a weighted sum of some number of inputs. This weighted sum is often passed through a non-linear ``activation'' function and the final result is referred to as the ``activation'' of the neuron. When values are supplied to the input neurons of the network, these values are forward-propagated through the network, activating each layer of neurons which, in turn, activate the next layer. The resulting activations of the neurons in the final layer are the output of the network.

\subsection{Convolutional Neural Networks}

All of the network architectures used throughout the project are convolutional neural networks (CNNs). A CNN is a type of neural network named after the discrete convolution operation that sets itself apart from typical fully connected ANNs. The convolution operation makes use of surrounding pixels in order to change the value of a central pixel. The 2D discrete convolution operation is defined as

\begin{equation}
    g(x,y)=\sum_{i=1}^{m}\sum_{j=1}^{n}f(x-j,y-k)h(j,k)
\end{equation}

\noindent
where $f$ is the input image, $h$ is an $m\times n$ kernel, and $g$ is the resulting image.

CNNs often contain multiple convolutional layers in which increasingly complex features of an image are detected by making use of a combination of simpler features detected in previous layers. A convolutional layer convolves the channels of an image with multiple learned kernels. A channel is simply a component of an image---an RGB image, for example, consists of three channels, whilst a greyscale image consists of only one. The output of these operations will be multiple modified images that are referred to as ``feature channels'' or ``feature maps''. These feature maps can then be used as the input channels to the next layer, and so on.

As well as the convolution operation, the max-pooling operation is often used in CNNs and plays an important role in the performance achieved by the CNNs experimented with throughout this project. The max-pooling operation is used to decrease the dimensionality of the input in an attempt to force the network to ``learn'' features present in the input. Max-pooling layers are also used by models to achieve some translation invariance over spatial shifts in the input image~\cite{segnet}. The max-pooling and convolution operations are illustrated in Figure \ref{fig:operations}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.43\textwidth}
        \centering
        \input{tikz/max_pool}
    \vspace*{1mm}
    \caption{2D Max-pooling}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.55\textwidth}
        \centering
        \input{tikz/conv}
        \vspace*{1mm}
        \caption{2D Convolution}
    \end{subfigure}
    \caption{Illustrations of the 2D max-pooling and convolution operations (own figure). \textbf{(a)} The max-pooling operation shown takes an input image $f$ and uses a 2$\times$2 filter and stride of two to produce the output image $g$. Note that the maximum value in each coloured region of the input is the resulting value of the corresponding regions of the output. \textbf{(b)} The convolution operation shown takes an input image $f$ and uses a 3$\times$3 kernel $h$ with a stride of one to produce the output image $g=f\ast h$, where $\ast$ denotes the convolution operation. Note that the kernel shown in the convolution operation is ``flipped'' in both the $x$ and $y$ axes before the element-wise multiplication and summation takes place.}
    \label{fig:operations}
\end{figure}

Another feature of CNNs that set them apart from their fully connected counterparts is the localised ``receptive fields'' of the neurons present in convolutional layers. Whilst neurons in a fully connected layer can receive input from every neuron in the previous layer, neurons present in a convolutional layer can only receive input from a small amount of neighbouring neurons in the previous layer. For example, if a 3$\times$3 kernel is used, a neuron's activation can only be influenced by nine neurons in the previous layer. This area of input that can influence a neuron is called its receptive field. This is one of the reasons why during hyperparameter optimisation, kernel sizes from various layers are often tuned, in order to increase or decrease the size of neurons' receptive fields. Hyperparameter optimisation is discussed in Section \ref{sec:hyperparam}.

\subsection{Backpropagation}
\label{sec:backprop}

First popularised by Rumelhart et al.\ in 1986~\cite{rumelhart}, the backpropagation algorithm is still the main learning mechanism used in neural networks today. Once a loss function is defined to measure the performance of the network, backpropagation can be used to compute the derivative of the loss function with respect to each weight in the network using the chain rule. The derivatives are calculated one layer at a time, iterating backward from the output layer. Since the backpropagation algorithm makes use of the chain rule, the loss function must be differentiable. This must also be the case for the chosen activation functions for each neuron.

An optimisation algorithm can then use these computed derivatives to adjust each weight in order to minimise the chosen loss function. Loss functions used throughout this project are discussed in Section \ref{sec:loss}.

\subsection{Optimisation Algorithms}

Once the derivatives---or ``gradients''---have been calculated, an optimisation algorithm is used to determine the exact value each weight should be updated to. Throughout the project, various optimisation algorithms are experimented with in order to improve the performance achieved. When training deep neural networks, some variant of the gradient descent algorithm is often used. Gradient descent~\cite[p. 536]{gradient} is an iterative algorithm designed to find local minima of some differentiable function---in this case, the loss function. A visualisation of the gradient descent algorithm is shown in Figure \ref{fig:gd}. The gradient descent algorithm evaluates the loss function over the entire dataset before taking a ``step'' in the direction of the gradient computed. A step consists of updating the value of every weight in order to decrease the average loss value that would be achieved by the network when processing the dataset. Evaluating the loss function over the entire dataset once is referred to as an ``epoch''.

The size of the weight update is not only determined by the gradient calculated, but also by a hyperparameter called the ``learning rate''. For example, gradient descent calculates the update for a single weight $w$ as

\begin{equation}
    w := w - \eta\nabla\ell(w)
\end{equation}

\noindent
where $\eta$ is the learning rate and $\nabla\ell(w)$ is the derivative of the loss function $\ell$ with respect to $w$. Choosing an appropriate learning rate is challenging but is essential to allow an optimisation algorithm to converge to a local minimum. A learning rate that is too large can cause the loss function to fluctuate around the minimum, impeding convergence or even causing divergence. The learning rate hyperparameter has a significant effect on training and is one of the most important parameters tuned in the hyperparameter optimisation process~\cite{bengio2012practical} (see Section \ref{sec:hyperparam}).

\subsubsection{Stochastic Gradient Descent}

Issues arise when using gradient descent with larger datasets, as evaluating the loss function over the entire dataset before a step can be taken becomes increasingly computationally costly~\cite{gdbad}. These issues have given rise to the popularity of the stochastic gradient descent (SGD) and the mini-batch gradient descent optimisation algorithms. SGD is a variant of the gradient descent algorithm that takes a step each time a sample is processed, rather than only once the entire dataset is processed. Mini-batch gradient descent is yet another variant of gradient descent in which weights are updated after evaluating the loss function over a subset---or ``batch''---of the training samples.

\begin{figure}[t]
    \centering
    \input{tikz/sgd}
    \caption{A diagram visualising the gradient descent algorithm for a model containing two trainable parameters, $x$ and $y$ (own figure). Given a random starting configuration with parameters $x_0$ and $y_0$, the average loss value that would be achieved when processing the entire dataset is given by $\ell(x_0, y_0)$. Say gradient descent takes $n$ steps in the direction of steepest descent at each iteration until the loss value converges to some local minimum, $x_n$ and $y_n$ would be the final parameter values and the final loss would be $\ell(x_n, y_n)$. It is worth noting that even though the two starting configurations shown in the diagram both converge to the same local minimum, this might not always be the case. A starting configuration nearer to one of the other possible local minima would most likely not converge to the example local minimum $\ell(x_n, y_n)$ shown.}
    \label{fig:gd}
\end{figure}

\subsubsection{Adam}

The Adam optimisation algorithm~\cite{adam} is the main algorithm utilised throughout the project. First introduced by Kingma and Ba in 2014, Adam is an optimisation algorithm that is also based off of SGD. However, rather than using the same learning rate across all parameters, Adam computes individual adaptive learning rates for each parameter. These individual learning rates are based off of estimates of the first moment (the mean) and second moment (the uncentered variance) of the gradients~\cite{gdbad}. Kingma and Ba showed empirically that Adam works well in practice and compares favourably to other optimisation algorithms when optimising both fully connected ANNs and deep CNNs.

\subsection{Loss functions}
\label{sec:loss}

A loss function takes both the ground truth label and the predicted label of a training sample, and outputs some measure of how well a model performed by producing that prediction. As mentioned in Section \ref{sec:backprop}, backpropagation makes use of the derivative of the loss function with respect to each parameter in order to minimise the loss achieved. If the appropriate loss function is chosen, minimising the loss should improve the performance achieved by the network. It is for this reason that loss functions must be differentiable. The loss functions that were experimented with throughout this project are outlined below.

\subsubsection{Binary Cross-Entropy Loss}

The binary cross-entropy loss function is used when classifying samples that can belong to two classes. Since the boundary extraction that this project attempts can be presented as a binary semantic segmentation task, the binary cross-entropy loss is a reasonable first choice of loss function. When performing binary image classification, a model will produce two probabilities---one for each class. Whereas, when performing binary semantic segmentation, a model effectively classifies each pixel; thus, two probabilities will be produced for each pixel. These are the model's predicted probabilities that a given image (or pixel) belongs to each of the two classes. Since these two classes are the only possible classes that the model can predict, the probabilities should sum to one. The binary cross-entropy loss is defined as:

\begin{equation}
    CE(p, y) = 
    \begin{cases}
        -\log(p) & \text{if } y = 1\\
        -\log(1 - p) & \text{otherwise}
    \end{cases}
\end{equation}

where $y \in \{+1, -1\}$ is the actual value (the ground-truth), and $p \in [0, 1]$ is the model's estimated probability for the class with label $y = 1$. This is the loss value achieved when classifying a single training sample (or a single pixel in the case of semantic segmentation).

\subsubsection{Binary Focal Loss}

Proposed by Lin et al.\ in 2018, the Focal Loss~\cite{focalloss} is designed to address the ``class imbalance'' problem. In a typical classification problem, class imbalance occurs when one class contains significantly fewer samples than the other classes. Lin et al.\ highlight the fact that when using the cross-entropy loss function, even samples that are well-classified incur a loss with ``non-trivial'' magnitude. When summed over large numbers of samples from the easily classified majority class, these small loss values can overwhelm the loss resulting from the minority class samples~\cite{focalloss}. As highlighted in Section \ref{sec:challenges}, the boundary extraction task inherently contains a severe class imbalance; the focal loss function could be used to reduce the negative effects of this class imbalance on the performance of the networks used throughout the project.

In an attempt to address the class imbalance problem, the focal loss introduces two new parameters: a weighting factor $\alpha \in [0, 1]$ and a ``focusing'' parameter $\gamma \geq 0$.
The binary focal loss is then defined as:

\begin{equation}
    \text{FL}(p, y) = 
    \begin{cases}
        -\alpha(1 - p)^{\gamma} \log (p) & \text{if } y = 1\\
        -\alpha(p^{\gamma}) \log (1 - p) & \text{otherwise}
    \end{cases}
\end{equation}

where $y \in \{+1, -1\}$ is the actual value (the ground-truth), and $p \in [0, 1]$ is the model's estimated probability for the class with label $y = 1$. The weighting factor increases the loss produced by the misclassification of minority class samples whilst the focusing factor ``reduces the contribution from easy examples and extends the range in which an example receives low loss''~\cite{focalloss}. When $\gamma = 0$ and $\alpha = 1$, the focal loss is equivalent to the cross-entropy loss. The contribution of the $\gamma$ value to the loss produced can be seen in Figure \ref{fig:focal}.

\begin{figure}[t]
    \centering
    \input{tikz/focal}
    \caption{An illustration of the focal loss with varying values of $\gamma$. Recreated from \cite[Fig. 1]{focalloss}: ``setting $\gamma > 0$ reduces the relative loss for well-classified examples $(p > 0.5)$, putting more focus on hard, misclassified examples''.}
    \label{fig:focal}
\end{figure}

\subsection{Accuracy Metrics}

Although loss functions can be used to measure performance, their main purpose is to be used to train the network. To quantify the performance achieved by a network, an accuracy metric should instead be used. Accuracy metrics do not play a direct role in training networks, though they can be used indirectly---for example, to determine whether or not to continue training. Choosing an appropriate accuracy metric for a given task is essential and proved challenging throughout this project due to the severe class imbalance present in the ground truth labels. A custom accuracy metric based off the Euclidean distances between points on the ground truth and predicted boundaries was conceived and is discussed further in Chapter \ref{chap:implementation}.

\subsection{Overfitting and Regularization}
\label{sec:regularization}

The term overfitting refers to the phenomenon when a model performs well on the data on which it is trained, yet poorly on data which it has not yet been exposed to. Overfitting is still a serious problem faced whilst training deep networks due to the large amount of parameters that must be learned~\cite{dropout, reducing, overfitavoid}. Regularization is any technique that is used in order to reduce overfitting and allow a model to generalise better~\cite{regular}. Due to the difficulties involved in manually labelling the coral data used in this project, obtaining enough labelled data proved challenging. \textbf{FINISH THIS} This section introduces some of the regularization techniques applied throughout the project.

\subsubsection{Data augmentation}

A very common technique used to reduce overfitting is data augmentation. Data augmentation is the process of augmenting the labelled training data in some way in order to increase the amount of training data available. This augmentation can be performed ``online'' with each training sample being randomly augmented during the training process, or it can be performed ``offline'' with the augmentation taking place before training. The training data can be augmented in many ways. Common examples for 2D images are: random rotations within some predefined range, random changes to brightness levels, and random horizontal and vertical flips. Effective data augmentation allows for a dataset to be artificially expanded, enabling it to represent a more comprehensive set of possible samples. Due to the challenges involved in labelling the data used in this project, data augmentation was an important regularization technique and was used to improve the performance of both the 2D and 3D models experimented with.

\subsubsection{Dropout}

Another technique often used to reduce overfitting is dropout. The use of dropout as a regularization technique when training neural networks was first popularised by Srivastava et al.~\cite{dropout} in 2014. Throughout the training process, subsets of randomly selected neurons (and their connections) are ``dropped''. Only the resulting reduced network is then trained on a particular training sample. A subset of dropped neurons will only remain dropped whilst processing a single training sample; the dropped neurons will then be added back to the network and a different subset of neurons will be dropped when processing the next training sample. The choice of which subset of neurons to drop for each training sample is random. 

Dropout reduces overfitting by preventing neurons from ``co-adapting'' too much~\cite{dropout}. Co-adaptation between neurons refers to the phenomenon when one neuron relies too heavily on the input from another neuron. If one neuron relies too heavily on the input of another neuron, the receiving neuron's performance in producing an activation that would be beneficial to the performance of the network may be hindered by the one ``bad'' input---even if the inputs from all other incoming neurons were ``good''. In this case, the terms good and bad are used to describe whether or not an input helped in contributing to the the beneficial activation value calculated via backpropagation.

\subsubsection{Cross-Validation}

When training and evaluating a model, the dataset is often split into three subsets: the training set, the validation set, and the test set. The training set is often the biggest subset and is used to fit the model. This is the set that the model ``learns'' from. The validation set is used to provide an unbiased evaluation of the model's performance on the dataset which can then be used to tune hyperparameters (discussed in Section \ref{sec:hyperparam}). Note that the evaluation of the model's performance provided by the validation set becomes more biased each time the validation performance is used to tune hyperparameters. It is important to keep the training and validation sets separate; the model must not be directly trained on any validation data. The test set is used to provide an unbiased evaluation of the final model. The test set is only used once the final model hyperparameters are chosen and the model has been trained. Evaluating the model's performance on a single test set may not be robust to ``selection biases''---biases that arise from selecting a particular test set. Although curating a test set that represents a wide range of possible data samples can help reduce selection bias, some amount of bias is still inevitable.

In order to reduce the effects of selection bias, a cross-validation technique can be used. Cross-validation is designed to give a comprehensive measure of a model's performance throughout the entire dataset, not just a particular subset.  Once the model's hyperparameters and training configuration have been finalised, multiple train/test splits can be used. For each new train/test split, the model can be re-trained and its performance will be evaluated on the new test set. This process can be repeated with multiple random train/test splits and the final reported performance will be the average of the performances achieved on each test set. In order to ensure that the final performance achieved is not biased, it is important to ensure that the model and its hyperparameters are not modified during the cross-validation process.

\subsubsection{Early Stopping}
Referred to as a ``beautiful free lunch'' by Geoffrey Hinton~\cite[p. 141]{earlystoppinglunch}, early stopping is a regularization technique in which the training process is stopped given some ``stopping criteria'' rather than after a set number of epochs. The simplest stopping criteria is to stop training as soon as the loss achieved on the validation set increases after an epoch rather than decreases. However, when training complex models, the ``loss curve'' achieved on the validation set may contain many local minima~\cite{earlystopping}. A more common method is to stop training once the loss achieved on the validation set fails to decrease after a certain number of epochs. This method helps to prevent noise in the validation loss curve from stopping the training process prematurely. A simple example of early stopping for an ideal loss curve is shown in Figure \ref{fig:earlystop}.

\begin{figure}[t]
    \centering
    \input{tikz/overfit}
    \caption{An illustration of idealised training and validation loss curves (own figure). Both the training and validation loss decrease until a point. However, from this point onward, although the training loss continues to decrease, the validation loss begins to increase and the model begins to overfit. This point at which the validation is lowest is the ideal time to stop the training process.}
    \label{fig:earlystop}
\end{figure}

\subsection{Ablation Studies}

In the context of deep neural networks, the term “ablation study” is used to describe a procedure in which certain parts of the network are removed in order to gain a better understanding of the network's behaviour. Ablation studies can also be used as a powerful optimisation tool. For example, if a certain layer or group of layers is removed and the performance of the network remains relatively unchanged, the ablated network would contain fewer parameters resulting in shorter training and inference times.

\subsection{Hyperparameter optimisation}
\label{sec:hyperparam}

A hyperparameter is a type of parameter which is explicitly set by hand as opposed to being learned via the training process. The learning rate and batch sizes are typical examples of hyperparameters, whereas neuron weights and the learned kernels used in convolutional layers are examples of parameters. The performance achieved by deep neural networks is known to depend critically on the identification of a good set of hyperparameters~\cite{hyperparam, goodhyperparam}.

The grid search technique has traditionally been used to find optimal hyperparameter configurations. A grid search is simply an exhaustive search through all of the possible combinations of a set of accepted hyperparameter values. In recent years, however, the grid search technique has often been replaced with a random search. Rather than exhaustively iterating over all combinations, random search iterates over random selections of combinations. Bergstra and Bengio~\cite{randomsearch} were able to show empirically that even over the same domain, random search is able to find hyperparameter configuratins that are as good as or better than configurations found via grid search. Bergstra and Bengio also demonstrated that random search can find these optimal configurations within a small fraction of the computation time taken by grid search.

\section{Network Architectures}

Various CNN architectures are experimented with throughout this project. This section introduces and details these architectures.

\subsection{SegNet}

In 2015, Badrinarayanan, Kendall, and Cipolla~\cite{segnet} introduced SegNet: a fully convolutional neural network architecture for semantic pixel-wise segmentation that was primarily motivated by road scene understanding applications. An architecture is ``fully convolutional'' when it contains no fully connected layers. The architecture consists of an ``encoder'' and a ``decoder'' with some extra pass through of information from layers early in the encoder to later layers in the decoder.

An encoder is a model or part of a model that takes some input and ``encodes'' the input into some lower dimensional representation. For example, a typical CNN architecture used for image classification is an example of an encoder, since the inputs are often high resolution two dimensional images, and the outputs are a small number of probabilities---one for each possible class that the image could be classified as. In fact, the encoder architecture used in SegNet is topologically identical to the convolutional layers used in the VGG16 architecture used for image classification~\cite{segnet, vgg16}. A decoder can be thought of as the opposite of an encoder; given some input, a decoder will ``decode'' the input back into some higher dimensional representation.

% The encoder consists of five blocks each containing multiple 3$\times$3 convolutional layers followed by a 2$\times$2 max-pooling layer. The decoder also consists of five of blocks, however, each block contains a 2$\times$2 upsample layer followed by multiple 3$\times$3 convolutional layers. The output of each convolutional layer is passed through a ReLU activation function.

The information passed through from the encoder to the decoder is in the form of max-pooling ``indices''. The decoder uses pooling indices computed in the max-pooling step of corresponding encoder layers to perform non-linear upsampling. These indices are simply the locations of the maximum value in each pooling window. This approach eliminates the need for the architecture to ``learn'' the indices to use to upsample with, since the indices that it will use are already learned in the max-pooling layers of the encoder. A diagram of the SegNet architecture is shown in Figure \ref{fig:segnet}.

\begin{figure}[t]
    \centering
    \hspace*{-0.3cm}
    \input{tikz/segnet}
    \caption[A diagram of the SegNet architecture (created using an open-source CNN architecture visualisation tool). Each cube represents a layer. The green arrows represent the feedforward of information from one layer to the next, whereas the blue arrows represent the passing forward of the max-pooling indices for the upsampling layers to use.]{A diagram of the SegNet architecture (created using an open-source CNN architecture visualisation tool\footnotemark). Each cube represents a layer. The green arrows represent the feedforward of information from one layer to the next, whereas the blue arrows represent the passing forward of the max-pooling indices for the upsampling layers to use.}
    \label{fig:segnet}
\end{figure}

\subsection{U-Net}

Presented by Ronneberger et al.\ in 2015~\cite{ronneberger2015u}, U-Net is a convolutional neural network architecture that was initially used to perform image segmentation on biomedical images, but has since been used on a wider range of visual data. At a high level, the U-Net architecture consists of three sections: the contracting path, the bottleneck, and the expanding path. The contracting path follows the typical architecture of a convolutional network, in that the image is gradually downsampled via convolutional and max-pooling layers and the number of feature channels is increased. The expansion path then gradually reconstructs the image via `up-convolutions'. Throughout the expansion path, the feature channels from the corresponding contraction layers are appended to the feature channels in the expansion layers. This allows the features that are learned whilst contracting the image to also be used to reconstruct it. A diagram of the U-Net architecture is shown later in Chapter \ref{chap:implementation} when its internals are discussed in more detail.

The U-Net and SegNet architectures share multiple similarities. They are both fully convolutional architectures designed to perform semantic pixel-wise segmentation. Both architectures also consist of an encoder, a bottleneck, and a decoder, with the encoder following a typical CNN architecture. Note that when describing the U-Net architecture, Ronneberger et al.\ refer to the encoder and decoder as the contracting and expanding paths respectively. Another similarity between the architectures is their passing of information from layers in the encoder to corresponding layers in the decoder. However, the passing of information is implemented differently in each architecture. Whilst U-Net passes information by concatenating feature maps from layers in the encoder to the feature maps of layers in the decoder, SegNet passes information in the form of pooling indices.

\footnotetext{\url{https://github.com/HarisIqbal88/PlotNeuralNet}}

\subsection{Generative Adversarial Networks}

A generative adversarial network (GAN)~\cite{gans} is a framework in which two models are pitted against each other: a generative model $G$ that captures the data distribution and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$. Both models are trained simultaneously, and in competition with each other, in that $G$ is trained to maximise the probability of $D$ making a mistake.

Many problems in image processing and computer vision involve ``image-to-image translation'': the translation of an input image into some corresponding output image. In recent years, adversarial nets have proved to be an effective general-purpose solution to image-to-image translation~\cite{gansoverview}. For example, the pix2pix model~\cite{pix2pix} demonstrates effective results in different computer vision tasks that had previously required special purpose models, including semantic segmentation and colourisation of black and white images.

% \section{Classical Image Processing Techniques}

% \subsection{Hausdorff distance}
% \label{sec:hausdorff}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.45\textwidth]{example-image-a}
%     \caption{Hausdorff illustration}
%     \label{fig:hausdorff}
% \end{figure}

\section{Supporting Technologies}

\subsection{Keras}

The Keras\footnote{\url{https://keras.io}} library is used to implement all architectures experimented with throughout this project. Keras is an open-source library written in Python that can be used to build and train deep learning models. Keras is capable of running on top of the TensorFlow\footnote{\url{https://tensorflow.org}} library allowing code to be run on both CPUs and GPUs.

Keras offers two APIs that can be used to define a model architecture: the sequential API and the functional API. The sequential API is easier to use and allows users to define a model architecture by simply passing a list of layer instances to the sequential constructor. However, the sequential API is limited in that it does not allow users to define architectures that share layers or have multiple inputs or outputs. Since the architectures implemented throughout this project contain layers that take inputs from more than one previous layer, the functional API must be used.

A simple example of how to define and train a model architecture using the Keras library is shown in Listing \ref{lst:keras}.
\input{listings/keras}